{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üÄÑ Ideogram-based vs. Phonogram-based Language\n#### Jason Heesang Lee","metadata":{}},{"cell_type":"code","source":"!pip install -q whoosh","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:12.991754Z","iopub.execute_input":"2023-09-13T01:00:12.992159Z","iopub.status.idle":"2023-09-13T01:00:30.441584Z","shell.execute_reply.started":"2023-09-13T01:00:12.992129Z","shell.execute_reply":"2023-09-13T01:00:30.440361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Disclaimer:* Initially, this project was just a quick review to fulfill my curiosity.<br>\nBut as I was developing this notebook, somehow it became a large-sized project..<br>\nI will try to finish this notebook by ***October 2023***.<br>\n\n------------------------------------------------------------------\n<br>\n\n***There*** are more than **7.8 billion** people in the world and with more than **7,000 languages**.<br><br>\nIn a greater perspective, there are two types of languages: Ideogram-Based Language and Phonogram-Based Language.<br>\nPhonogram-based Languages are languages that are developed on phonemes (speech sound) or a combination of phonemes.<br>\nLatin alphabets and Korean (Hangul) are examples.<br>\n<br>\nIdeogram Based Languages are the languages that are developed on symbols of writing systems.\nChinese, Egyptian Hieroglyph and Sumero-Akkadian Cuneiform are examples.<br> [Source: Wikipedia](https://en.wikipedia.org/wiki/Ideogram)<br>\n<br>\nAs I am fluent in Korean, English, and Chinese, I was suddenly curious about the possible differences in Natural Language Processing (NLP) techniques dealing with these two types of languages.<br> [Source: Wikipedia](https://en.wikipedia.org/wiki/Phonogram_(linguistics))<br>\n<br>\nIn a brief thoughts, I believe it is easier to process Ideogram Based Languages than Phonogram Based Languages.<br>\n<br>\nIt is due to the characteristics of the Ideogram Based Languages.\n<br>\nTaking Chinese (which I am familiar with) as an example, each character represents a certain definition. Each character (or an alphabet) in Phonogram Based Languages such as English and Hangul, often needs other characters to contain a definition.<br>\n<br>\n**`Hypothesis`** : Ideogram-based Languages might not need special Tokenizations or Embeddings for Natural Language Processing.<br>\n<br>\nI tried to ask and discuss with the lecturers here at Year-Dream School (Data Science Bootcamp) regarding this topic.<br>\nI only had a meaningful discussion with [@Yongdam Kim](https://www.kaggle.com/emphymachine) as he and some of his friend has some (not a lot, as per he claims) experience in this field.<br>\nHe mentioned that Natural Language Processing can be easier for Ideogram-based languages, as each character in this language contains meaning, which already could be similar to embedding.<br>\n<br>\nAs I want to further research into this topic, I had to first ask ChatGPT and Google Bard to fulfill my curiosity.<br>\n<br>\n***My query was as below.***<br>\n\n> *I was recently wondering that NLP process could be different between Phonogram based languages like English, and Ideogram based language like Chinese.<br>\nLike Tokenization, Embedding, Vectorization, Lemmatization, Stemming, etc.<br>\nCould you tell me the main differences in process of Natural Language Processing?*\n>\n\nBelow are the responses from the LLMs (redirected to my Notion page)<br><br>\n**`ChatGPT`**<br>\n[GPT - NLP Phonogram Ideogram.pdf](https://www.notion.so/jason-heesang-lee/Ideogram-Based-Language-vs-Phonogram-Based-Language-6ba064e320e2413aaba60f6aba5e6e19?pvs=4#b378a20a10ca4580b4442a5a4486b87f)<br>\n<br>\n**`Google Bard`**<br>\n[Bard - NLP Phonogram Ideogram.pdf](https://www.notion.so/jason-heesang-lee/Ideogram-Based-Language-vs-Phonogram-Based-Language-6ba064e320e2413aaba60f6aba5e6e19?pvs=4#11e5c642f4394e1c82311677d4ea1268)<br>\n<br>\nThere were some points that were interesting.<br>\n<br>\nFirst, both GPT and Bard told me that the Tokenization process might be harder on Ideogram-based Languages.<br>\nTokenization is the process of decomposing a sentence into words.<br>\nAs each word is represented in a way as a sequence of characters, it would be easier for the tokenizing process.<br>","metadata":{}},{"cell_type":"markdown","source":"My plan is to open up each key modules and figure out how they work.<br>\nFor Jieba, I will try to understand how this module is able to perform such segmentation.<br>\nAlso for DeBERTa Tokenizer or AutoTokenizer (I need to find out which module makes the difference), I want to know the inner logic that processes English and Chinese with the same lines of code.<br>","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/sentencepiece-pb2/\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\nimport re\nimport nltk\nimport jieba\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport sentencepiece_pb2\nimport sentencepiece as spm\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:30.443612Z","iopub.execute_input":"2023-09-13T01:00:30.444010Z","iopub.status.idle":"2023-09-13T01:00:34.977715Z","shell.execute_reply.started":"2023-09-13T01:00:30.443976Z","shell.execute_reply":"2023-09-13T01:00:34.976801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\nI have brought [Chinese Daily News](https://www.kaggle.com/datasets/noxmoon/chinese-official-daily-news-since-2016) by [@noxmoon](https://www.kaggle.com/noxmoon) & True news from [Fake and Real News](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset) by [@clmentbisaillon](https://www.kaggle.com/clmentbisaillon) to compare the process.<br>","metadata":{}},{"cell_type":"code","source":"cn_df = pd.read_csv('/kaggle/input/chinese-official-daily-news-since-2016/chinese_news.csv', encoding='utf-8')\ndisplay(cn_df.head(5))\nprint()\nen_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\ndisplay(en_df.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:34.978977Z","iopub.execute_input":"2023-09-13T01:00:34.980478Z","iopub.status.idle":"2023-09-13T01:00:36.885229Z","shell.execute_reply.started":"2023-09-13T01:00:34.980431Z","shell.execute_reply":"2023-09-13T01:00:36.884023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking DataFrame Information","metadata":{}},{"cell_type":"code","source":"print(f\"cn_df.info() :\\n{cn_df.info()}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.887992Z","iopub.execute_input":"2023-09-13T01:00:36.888503Z","iopub.status.idle":"2023-09-13T01:00:36.931913Z","shell.execute_reply.started":"2023-09-13T01:00:36.888470Z","shell.execute_reply":"2023-09-13T01:00:36.930416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"en_df.info() :\\n{en_df.info()}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.933651Z","iopub.execute_input":"2023-09-13T01:00:36.935459Z","iopub.status.idle":"2023-09-13T01:00:36.961864Z","shell.execute_reply.started":"2023-09-13T01:00:36.935415Z","shell.execute_reply":"2023-09-13T01:00:36.960481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dropping unnecessary columns\nI will drop date & tag columns from each DataFrame.<br>\nAnd matched the column names.","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"cn_df = cn_df.drop(columns=['date', 'tag'])\nen_df = en_df.drop(columns=['date', 'subject'])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.963785Z","iopub.execute_input":"2023-09-13T01:00:36.964121Z","iopub.status.idle":"2023-09-13T01:00:36.980463Z","shell.execute_reply.started":"2023-09-13T01:00:36.964091Z","shell.execute_reply":"2023-09-13T01:00:36.979531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"list(cn_df.columns) :\\n{list(cn_df.columns)}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.982277Z","iopub.execute_input":"2023-09-13T01:00:36.983038Z","iopub.status.idle":"2023-09-13T01:00:36.989771Z","shell.execute_reply.started":"2023-09-13T01:00:36.982992Z","shell.execute_reply":"2023-09-13T01:00:36.988507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_df = en_df.rename(columns={'title':'headline', 'text':'content'})\nprint(f\"list(en_df.columns) :\\n{list(en_df.columns)}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.991466Z","iopub.execute_input":"2023-09-13T01:00:36.991917Z","iopub.status.idle":"2023-09-13T01:00:37.013632Z","shell.execute_reply.started":"2023-09-13T01:00:36.991875Z","shell.execute_reply":"2023-09-13T01:00:37.010073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Most of NLP Technique retrieved from [@jhoward](https://www.kaggle.com/jhoward)'s notebook\n***[Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)***","metadata":{}},{"cell_type":"code","source":"cn_example = pd.DataFrame(cn_df.iloc[0]).T\nprint(f\"cn_example :\\n{cn_example}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:37.015378Z","iopub.execute_input":"2023-09-13T01:00:37.016439Z","iopub.status.idle":"2023-09-13T01:00:37.033530Z","shell.execute_reply.started":"2023-09-13T01:00:37.016403Z","shell.execute_reply":"2023-09-13T01:00:37.032241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_example = pd.DataFrame(en_df.iloc[0]).T\nprint(f\"en_example :\\n{en_example}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:37.038027Z","iopub.execute_input":"2023-09-13T01:00:37.038946Z","iopub.status.idle":"2023-09-13T01:00:37.049388Z","shell.execute_reply.started":"2023-09-13T01:00:37.038900Z","shell.execute_reply":"2023-09-13T01:00:37.047949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CN Text Preprocessing\n##### CN Definition retrieved from [Baidu Wenku](https://wenku.baidu.com/view/039d6d4e551252d380eb6294dd88d0d233d43cc8.html?_wkts_=1693548615550&bdQuery=%E4%B8%AD%E6%96%87+%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80+%E5%89%8D%E5%A4%84%E7%90%86)","metadata":{}},{"cell_type":"code","source":"# stopwords = [k.strip() for k in open('/kaggle/input/english-and-chinese-stopwords/stopwords.txt', encoding='utf8').readlines() if k.strip() != '']\n\ndef find_chinese(text):\n    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n    chinese_txt = re.sub(pattern,'',text)\n    return str(chinese_txt)\n\ndef cut_words(text):\n    jieba_txt = ' '.join(jieba.cut(find_chinese(text), cut_all=False))\n    return jieba_txt\n\n# def seg_sentence(text_list):\n#     seg_text = [word for word in text_list if word not in stopwords]\n#     return seg_text","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:37.051028Z","iopub.execute_input":"2023-09-13T01:00:37.052382Z","iopub.status.idle":"2023-09-13T01:00:37.064088Z","shell.execute_reply.started":"2023-09-13T01:00:37.052330Z","shell.execute_reply":"2023-09-13T01:00:37.062629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile cn_text.txt\n ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:37.066147Z","iopub.execute_input":"2023-09-13T01:00:37.067469Z","iopub.status.idle":"2023-09-13T01:00:37.080729Z","shell.execute_reply.started":"2023-09-13T01:00:37.067420Z","shell.execute_reply":"2023-09-13T01:00:37.079737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cn_text_file = open('/kaggle/working/cn_text.txt', 'w')\ncn_text = ''\nfor column in cn_example.columns:\n    temp = []\n    for row in tqdm(range(cn_example.shape[0])):\n        text = cn_example.iloc[row][column]\n        text = cut_words(text)\n        \n        temp.append(text)\n        cn_text = cn_text + '; ' + text\n#     text = seg_sentence(temp)\n    cn_example[column] = pd.Series(temp)\n\n#     cn_example[column] = pd.Series(seg_sentence(temp))\ncn_text_file.write(cn_text)\ndisplay(cn_example.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:37.082214Z","iopub.execute_input":"2023-09-13T01:00:37.082754Z","iopub.status.idle":"2023-09-13T01:00:38.531169Z","shell.execute_reply.started":"2023-09-13T01:00:37.082721Z","shell.execute_reply":"2023-09-13T01:00:38.529765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EN Text Preproecessing\nI will drop the names of the news companies.","metadata":{}},{"cell_type":"code","source":"%%writefile en_text.txt\n ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:38.532917Z","iopub.execute_input":"2023-09-13T01:00:38.533438Z","iopub.status.idle":"2023-09-13T01:00:38.541029Z","shell.execute_reply.started":"2023-09-13T01:00:38.533391Z","shell.execute_reply":"2023-09-13T01:00:38.539521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = []\nen_text_file = open('/kaggle/working/en_text.txt', 'w')\n\nen_text = ''\nfor row in tqdm(range(en_example.shape[0])):\n    text_h = en_df.headline[row]\n    text = \" \".join(en_example.content[row].split(' - ')[1:])\n    en_text = en_text + '; ' + text_h\n    en_text = en_text + \"; \" + text\n    temp.append(text)\n\nen_text_file.write(en_text)\nen_example.content = pd.Series(temp)\ndisplay(en_example.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.543285Z","iopub.execute_input":"2023-09-13T01:00:38.543772Z","iopub.status.idle":"2023-09-13T01:00:38.570055Z","shell.execute_reply.started":"2023-09-13T01:00:38.543726Z","shell.execute_reply":"2023-09-13T01:00:38.568415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking text files","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/cn_text.txt') as cn_text_file:\n    print(cn_text_file.read())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.572310Z","iopub.execute_input":"2023-09-13T01:00:38.572820Z","iopub.status.idle":"2023-09-13T01:00:38.581140Z","shell.execute_reply.started":"2023-09-13T01:00:38.572773Z","shell.execute_reply":"2023-09-13T01:00:38.579581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/en_text.txt') as en_text_file:\n    print(en_text_file.read())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.583006Z","iopub.execute_input":"2023-09-13T01:00:38.583507Z","iopub.status.idle":"2023-09-13T01:00:38.598959Z","shell.execute_reply.started":"2023-09-13T01:00:38.583461Z","shell.execute_reply":"2023-09-13T01:00:38.597452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Jieba\n***I guess this is where I have to examine the [Jieba Github](https://github.com/fxsjy/jieba)...!***\n\n**This is how the repository looks like.**<br>\n<img src=\"https://github.com/jasonheesanglee/Ideogram_Phonogram/blob/main/IDEOPHONO/jieba_main.png?raw=true\" height=\"100\" /><br>\nThere are 3 different directories - extra_dict, jieba, test, and some config files.<br>\nLet's first look into README.md to grasp the concept of what this module is in the end.<br>\nI brought English version of README.<br>\n(The content is identical with the Chinese version.)","metadata":{}},{"cell_type":"markdown","source":"## jieba\n-----------------------------------\n\n*Jieba (Chinese for \"to stutter\") Chinese text segmentation: built to be the best Python Chinese word segmentation module.*<br>\n***This is the explanation of what jieba is***<br>","metadata":{}},{"cell_type":"markdown","source":"### Features\n-----------------------------------\n- Support three types of segmentation mode:\n1. Accurate Mode attempts to cut the sentence into the most accurate segmentations, which is suitable for text analysis.\n2. Full Mode gets all the possible words from the sentence. Fast but not accurate.\n3. Search Engine Mode, based on the Accurate Mode, attempts to cut long words into several short words, which can raise the recall rate. Suitable for search engines.\n- Supports Traditional Chinese\n- Supports customized dictionaries\n- MIT License\n\n***There are 3 different segmentation modes, and the usage of each mode differs from the purpose of the usage.***","metadata":{}},{"cell_type":"markdown","source":"### Online demo \n-----------------------------------\n\n[http://jiebademo.ap01.aws.af.cm/](http://jiebademo.ap01.aws.af.cm/)<br>\n***This online demo is not working anymore (404 Error)***","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Usage\n-----------------------------------\n\n- Fully automatic installation: `easy_install jieba` or `pip install jieba`<br>\n- Semi-automatic installation: Download [http://pypi.python.org/pypi/jieba/](https://pypi.org/project/jieba/) , run `python setup.py install` after extracting.<br>\n- Manual installation: place the `jieba` directory in the current directory or python `site-packages` directory.<br>\n- `import jieba`.<br>\n\n***This section explains how to import the module***","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Algorithm\n-----------------------------------\n\n- Based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations.\n- Use dynamic programming to find the most probable combination based on the word frequency.\n- For unknown words, a HMM-based model is used with the Viterbi algorithm.\n\n***Wait, there are so many terms I have no clue about.<br>What is Directed Acyclic Graph? <br>What is HMM-based model? <br>What is Viterbi algorithm?***\n\n#### Directed Acyclic Graph (DAG)\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Tred-G.svg/1920px-Tred-G.svg.png\" width=200 />\n\nBased on [Wikipedia](https://en.wikipedia.org/wiki/Directed_acyclic_graph), Directed Acyclic Graph is a \n>*Directed graph with no directed cycles. A directed graph is a DAG if and only if it can be [topologically ordered](https://en.wikipedia.org/wiki/Topological_order), by arranging the vertices as a linear ordering that is consistent with all edge directions.* <br>\n\nWhat? still no clue yet, let's move on to the definitions.<br>\n\n> *A graph is formed by vertices and by edges connecting pairs of vertices, where the vertices can be any kind of object that is connected in pairs by edges. In the case of a directed graph, each edge has an orientation, from one vertex to another vertex. A path in a directed graph is a sequence of edges having the property that the ending vertex of each edge in the sequence is the same as the starting vertex of the next edge in the sequence; a path forms a cycle if the starting vertex of its first edge equals the ending vertex of its last edge. A directed acyclic graph is a directed graph that has no cycles.*<br><br>\n\n**TL;DR** (I shouldn't though)\n\nInstead of learning it from Wikipedia, I searched Google a bit more and completely understood the concept from [StackExchange](https://math.stackexchange.com/questions/3782987/difference-between-oriented-graph-and-directed-acyclic-graphs-dag#:~:text=Basically%20directed%20graphs%20can%20have,two%20vertices%20A%20and%20B.&text=In%20mathematics%2C%20particularly%20graph%20theory,graph%20with%20no%20directed%20cycles.)<br>\nPlease correct me if I am wrong;<br>\nBasically DAG is a graph of number of vertices connected by edges (with direction), and this edge doesn't go back but only go forth.<br> Which makes this graph a graph with direction, but not circulating.<br>\n***OH*** That is why its name is **Directed** **A**cyclic Graph!!\n\n#### HMM-based model\nBased on this [article](https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4) by [Mehul Gupta](https://medium.com/@mehulgupta_7991), to understand the conecept of Hidden Markov Model (HMM)-based model, we need to understand what ***Markov Chain*** is.<br>\nIt gave a simple definition of Markov chain and I completely got it!\n> *A Markov chain is a model that tells us something about the probabilities of sequences of random states/variables. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state.*\n\nBelow is an example the writer gave, and I believe this is just a perfect example.\n> *A Markov Chain model based on Weather might have Hot, Cool, and Rainy as its states & to predict tomorrow‚Äôs weather you could examine today‚Äôs weather but yesterday‚Äôs weather isn‚Äôt significant in the prediction.*<br>\n\nBelow are specified all the components of Markov Chains.\n<img src=\"https://miro.medium.com/v2/resize:fit:1104/format:webp/1*tI5HGo_cFTxgcxiEDHQMOw.png\" height=100 />\n\nMoving on to ***HMM-based model***.<br>\n> *Sometimes, what we want to predict is a sequence of states that aren‚Äôt directly observable in the environment. Though we are given another sequence of states that are observable in the environment, these hidden states have some dependence on the observable states.*\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXjrDa28pUnGmI0ehjhR8A.png\" height=100 /><br>\n> *In the above HMM, we are given Walk, Shop & Clean as observable states. But we are more interested in tracing the sequence of the hidden states that will be followed which are Rainy & Sunny.*<br>\n\n***As per my understanding, simply saying, this is an advanced step after Markov Chain.<br>\nAccording to the provided image, actions are the subjects that we are predicting, and the original factors of Markov Chain example (weather conditions), are the hidden layer (state) that influences the prediction of the action.***<br>\n\nHidden Markov Model is needed for Part of Speech Tagging (Categories of words; verbs, nouns, actions, expresssions and so on).<br>\n> If you notice closely, we can have the words in a sentence as Observable States (given to us in the data) but their POS Tags as Hidden states and hence we use HMM for estimating POS tags. It must be noted that we call Observable states ‚ÄòObservation‚Äô & Hidden states ‚ÄòStates‚Äô.\n\nBelow are the specified all the components of HMM<br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1240/format:webp/1*ARltONawvqjzKeZOMvD-tg.png\" height=100 /><br>\n> *$Q$: Set of possible Tags<br><br>\n$A$: The A matrix contains the tag transition probabilities<br><br>\n$P$($ti$|$ti‚àí1$) which represent the probability of a tag occurring given the previous tag. Example: Calculating A[Verb][Noun]:\n$P$ (Noun|Verb): Count(Noun & Verb)/Count(Verb)<br><br>\n$O$: Sequence of observation (words in the sentence)<br><br>\n$B$: The $B$ emission probabilities, $P(wi|ti)$, represent the probability, given a tag (say Verb), that it will be associated with a given word (say Playing). The emission probability $B$[Verb][Playing] is calculated using:<br><br>\n$P$(Playing | Verb): Count (Playing & Verb)/ Count (Verb)<br><br>\nIt must be noted that we get all these Count() from the corpus itself used for training.<br><br>\nA sample HMM with both ‚ÄòA‚Äô & ‚ÄòB‚Äô matrices will look like this :*\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1026/format:webp/1*TY_h8WgfRH7iJy1PZKN5UQ.png\" height=100 />\n\n> *Here, the black, straight arrows represent values of Transition matrix ‚ÄòA‚Äô while the dotted black arrow represents Emission Matrix ‚ÄòB‚Äô for a system with Q: {MD, VB, NN}.*<br>\n\nThe writer has also explained about Decoding using HMMs.<br>\nWhile skimming through the upcoming content (Viterbi Algorithm) of the same writer, I thought it was necessary to go through this part as well.<br>\n> Given an input as HMM (Transition Matrix, Emission Matrix) and a sequence of observations $O = o1, o2, ‚Ä¶, oT$ (Words in sentences of a corpus), find the most probable sequence of states $Q = q1q2q3‚Ä¶, qT$ (POS Tags in our case)<br>\nThe two major assumptions followed while decoding tag sequence using HMMs:\n> - The probability of a word appearing depends only on its **own tag** and is independent of neighboring words and tags.\n> - The probability of a tag depends only on ***the previous tag(bigram HMM)*** that occured rather than the entire previous tag sequence i.e. shows Markov Property. Though we can be flexible with this.\n\nWhich, I believe, means unlikely to Markov Chain, it references on the previous tag, but surely, not the entire previous tags.<br>\nLet's move on to Viterbi Algorithm\n\n### Viterbi Algorithm\n[Mehul Gupta](https://medium.com/@mehulgupta_7991) has also well explained about Viterbi Algorithm from the same [article](https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4).<br><br>\nViterbi Algorithm is a decoding algorithm used for HMMs.<br>\nThe writer mentioned that setting up Lattice, the probability matrix, is necessary.<br>\nIn prior to proceed further, being familiar with the tags of Part of Speech would be necessary.<br><br>\n\n<img src=\"https://m-clark.github.io/text-analysis-with-R/img/POS-Tags.png\" height=200 /> <br>\n[***Source: Text Analysis in R by Michael Clark***](https://m-clark.github.io/text-analysis-with-R/part-of-speech-tagging.html)<br>\n\n    \nWith a sample sentence ***Janet will back the bill***, it will look like this on Lattice:<br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1080/format:webp/1*8-5KZVj-_jZOWN83gGhD5A.png\" height=100 />\n\nAs all the words in this sentence are commonly used words, there are no word with an \"Unknown\" tag.<br><br>\n\nEach cell of the lattice is represented by $V_t(j)$, $V$ for Viterbi, $t$ for column, $j$ for row.<br>\nThis represents probability that the HMM is in $state j(present POS Tag)$ after seeing the $first t observations (past words for which lattice values has been calculated)$.<br>\nThis passes through the most **probable state sequence (Previous POS Tag)** $q_1, q_2, ... q_t-1$.<br>\nWhich means, if we have the word **back**, we most probably will have **Janet** and **will** in previous order.<br>\n\n$V_t(j)$ is calculated as :\n$$V_t(j) = max: V_t-1*a(i,j)* b_j(O_t)$$\nwhere we got ‚Äòa‚Äô(transition matrix) & ‚Äòb‚Äô(emission matrix) from the HMM part calculations discussed above.:<br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1014/format:webp/1*1UylhpDw7suhH9WpnPYFaw.png\" height=20 />","metadata":{}},{"cell_type":"markdown","source":"### Main Functions\n-----------------------------------\n\n### 1. Cut\n-----------------------------------\n***This section shows how to use jieba.cut method.***<br>\n- The `jieba.cut` function accepts three input parameters: the first parameter is the string to be cut; the second parameter is `cut_all`, controlling the cut mode; the third parameter is to control whether to use the Hidden Markov Model.\n- `jieba.cut_for_search` accepts two parameter: the string to be cut; whether to use the Hidden Markov Model. This will cut the sentence into short words suitable for search engines.\n- The input string can be an unicode/str object, or a str/bytes object which is encoded in UTF-8 or GBK. Note that using GBK encoding is not recommended because it may be unexpectly decoded as UTF-8.\n- `jieba.cut` and `jieba.cut_for_search` returns an generator, from which you can use a `for` loop to get the segmentation result (in unicode).\n- `jieba.lcut` and `jieba.lcut_for_search` returns a list.\n- `jieba.Tokenizer(dictionary=DEFAULT_DICT)` creates a new customized Tokenizer, which enables you to use different dictionaries at the same time. `jieba.dt` is the default Tokenizer, to which almost all global functions are mapped.\n<br>","metadata":{}},{"cell_type":"code","source":"print(\"Code example: Segmentation\\n\\nOutput: \")\n\n#encoding=utf-8\nimport jieba\n\nseg_list = jieba.cut(\"ÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶\", cut_all=True)\nprint(\"Full Mode: \" + \"/ \".join(seg_list))  # ÂÖ®Ê®°Âºè\nprint()\nseg_list = jieba.cut(\"ÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶\", cut_all=False)\nprint(\"Default Mode: \" + \"/ \".join(seg_list))  # ÈªòËÆ§Ê®°Âºè\nprint()\nseg_list = jieba.cut(\"‰ªñÊù•Âà∞‰∫ÜÁΩëÊòìÊù≠Á†îÂ§ßÂé¶\")\nprint(\", \".join(seg_list))\nprint()\nseg_list = jieba.cut_for_search(\"Â∞èÊòéÁ°ïÂ£´ÊØï‰∏ö‰∫é‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊâÄÔºåÂêéÂú®Êó•Êú¨‰∫¨ÈÉΩÂ§ßÂ≠¶Ê∑±ÈÄ†\")  # ÊêúÁ¥¢ÂºïÊìéÊ®°Âºè\nprint(\", \".join(seg_list))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:38.600752Z","iopub.execute_input":"2023-09-13T01:00:38.601119Z","iopub.status.idle":"2023-09-13T01:00:38.616458Z","shell.execute_reply.started":"2023-09-13T01:00:38.601089Z","shell.execute_reply":"2023-09-13T01:00:38.615137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Add a custom dictionary\n-----------------------------------\n***This section explains about how to load & modifying the dictionary***<br>\n#### Load dictionary\n- Developers can specify their own custom dictionary to be included in the jieba default dictionary. Jieba is able to identify new words, but you can add your own new words can ensure a higher accuracy.\n- Usage: `jieba.load_userdict(file_name)` # file_name is a file-like object or the path of the custom dictionary\n- The dictionary format is the same as that of `dict.txt`: one word per line; each line is divided into three parts separated by a space: word, word frequency, POS tag. If `file_name` is a path or a file opened in binary mode, the dictionary must be UTF-8 encoded.\n- The word frequency and POS tag can be omitted respectively. The word frequency will be filled with a suitable value if omitted.\n\n**Example:** <br>\n*ÂàõÊñ∞Âäû 3 i*<br>\n*‰∫ëËÆ°ÁÆó 5<br>\nÂá±ÁâπÁê≥ nz<br>\nÂè∞‰∏≠<br>*\n\n- Change a Tokenizer's `tmp_dir` and `cache_file` to specify the path of the cache file, for using on a restricted file system.\n\n**Example:** <br>\n*‰∫ëËÆ°ÁÆó 5<br>\n  ÊùéÂ∞èÁ¶è 2<br>\n  ÂàõÊñ∞Âäû 3<br>\n  [Before]Ôºö ÊùéÂ∞èÁ¶è / ÊòØ / ÂàõÊñ∞ / Âäû / ‰∏ª‰ªª / ‰πü / ÊòØ / ‰∫ë / ËÆ°ÁÆó / ÊñπÈù¢ / ÁöÑ / ‰∏ìÂÆ∂ /<br>\n  [After]Ôºö„ÄÄÊùéÂ∞èÁ¶è / ÊòØ / ÂàõÊñ∞Âäû / ‰∏ª‰ªª / ‰πü / ÊòØ / ‰∫ëËÆ°ÁÆó / ÊñπÈù¢ / ÁöÑ / ‰∏ìÂÆ∂ /*<br>\n  \n#### Modify dictionary\n- Use add_word(word, freq=None, tag=None) and del_word(word) to modify the dictionary dynamically in programs.\n- Use suggest_freq(segment, tune=True) to adjust the frequency of a single word so that it can (or cannot) be segmented.\n- Note that HMM may affect the final result.\n","metadata":{}},{"cell_type":"code","source":"print(\"Example :\\n\")\n>>> print('/'.join(jieba.cut('Â¶ÇÊûúÊîæÂà∞post‰∏≠Â∞ÜÂá∫Èîô„ÄÇ', HMM=False)))\n# Â¶ÇÊûú/ÊîæÂà∞/post/‰∏≠Â∞Ü/Âá∫Èîô/„ÄÇ\n>>> jieba.suggest_freq(('‰∏≠', 'Â∞Ü'), True)\n# 494\n>>> print('/'.join(jieba.cut('Â¶ÇÊûúÊîæÂà∞post‰∏≠Â∞ÜÂá∫Èîô„ÄÇ', HMM=False)))\n# Â¶ÇÊûú/ÊîæÂà∞/post/‰∏≠/Â∞Ü/Âá∫Èîô/„ÄÇ\n>>> print('/'.join(jieba.cut('„ÄåÂè∞‰∏≠„ÄçÊ≠£Á°ÆÂ∫îËØ•‰∏ç‰ºöË¢´ÂàáÂºÄ', HMM=False)))\n# „Äå/Âè∞/‰∏≠/„Äç/Ê≠£Á°Æ/Â∫îËØ•/‰∏ç‰ºö/Ë¢´/ÂàáÂºÄ\n>>> jieba.suggest_freq('Âè∞‰∏≠', True)\n# 69\n>>> print('/'.join(jieba.cut('„ÄåÂè∞‰∏≠„ÄçÊ≠£Á°ÆÂ∫îËØ•‰∏ç‰ºöË¢´ÂàáÂºÄ', HMM=False)))\n# „Äå/Âè∞‰∏≠/„Äç/Ê≠£Á°Æ/Â∫îËØ•/‰∏ç‰ºö/Ë¢´/ÂàáÂºÄ\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:38.618894Z","iopub.execute_input":"2023-09-13T01:00:38.619609Z","iopub.status.idle":"2023-09-13T01:00:38.632659Z","shell.execute_reply.started":"2023-09-13T01:00:38.619560Z","shell.execute_reply":"2023-09-13T01:00:38.631156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Keyword Extraction\n-----------------------------------\n***This section explains how to extract keywords***<br>\n\n`import jieba.analyse`\n\n- `jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())`<br>\n    - `sentence`: the text to be extracted<br>\n    - `topK`: return how many keywords with the highest TF/IDF weights. The default value is 20<br>\n`withWeight`: whether return TF/IDF weights with the keywords. The default value is False<br>\n    - `allowPOS`: filter words with which POSs are included. Empty for no filtering.<br>\n- `jieba.analyse.TFIDF(idf_path=None)` creates a new TF/IDF instance, `idf_path` specifies IDF file path.<br><br>\n\n**Example (keyword extraction)**<br>\nhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags.py<br>\nDevelopers can specify their own custom IDF corpus in jieba keyword extraction<br>\n- Usage: `jieba.analyse.set_idf_path(file_name)`<br>\n`file_name` is the path for the custom corpus<br>\n- Custom Corpus Sample: (not working)<br>\nhttps://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big\n- Sample Code:<br>\nhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py<br>\nDevelopers can specify their own custom stop words corpus in jieba keyword extraction\n\n- Usage: `jieba.analyse.set_stop_words(file_name)`<br>\n`file_name` is the path for the custom corpus\n- Custom Corpus Sample:<br>\nhttps://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt\n- Sample Code:<br>\nhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py<br>\n\nThere's also a [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) implementation available.\n\n- Use: `jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))`\n\nNote that it filters POS by default.<br>\n`jieba.analyse.TextRank()` creates a new TextRank instance.\n","metadata":{}},{"cell_type":"markdown","source":"### 4. Part of Speech Tagging\n-----------------------------------\n***This section explains how to tag Part of Words***<br>\n\n- `jieba.posseg.POSTokenizer(tokenizer=None)` creates a new customized Tokenizer.<br>\n`tokenizer` specifies the `jieba.Tokenizer` to internally use. jieba.posseg.dt is the default POSTokenizer.\n- Tags the POS of each word after segmentation, using labels compatible with ictclas.<br>\n\n**Example:**","metadata":{}},{"cell_type":"code","source":">>> import jieba.posseg as pseg\n>>> words = pseg.cut(\"ÊàëÁà±Âåó‰∫¨ÂõΩÂÆâ\")\n>>> for w in words:\n...    print('%s %s' % (w.word, w.flag))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.634981Z","iopub.execute_input":"2023-09-13T01:00:38.635484Z","iopub.status.idle":"2023-09-13T01:00:39.354590Z","shell.execute_reply.started":"2023-09-13T01:00:38.635441Z","shell.execute_reply":"2023-09-13T01:00:39.353328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Parallel Processing\n-----------------------------------\n***This section explains about parallel processing.***<br><br>\nI believe this would be helpful on large dataset, but I will not implement this module on this notebook.<br>\n\n- Principle: Split target text by line, assign the lines into multiple Python processes, and then merge the results, which is considerably faster.\n\n- Based on the multiprocessing module of Python.\n\n- Usage:\n\n    - `jieba.enable_parallel(4)`<br>\n    Enable parallel processing. The parameter is the number of processes.\n    - `jieba.disable_parallel()`<br>\n    Disable parallel processing.\n    \n- **Example:** https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py\n\n- Result: On a four-core 3.4GHz Linux machine, do accurate word segmentation on Complete Works of Jin Yong, and the speed reaches 1MB/s, which is 3.3 times faster than the single-process version.\n\n- Note that parallel processing supports only default tokenizers, `jieba.dt` and `jieba.posseg.dt`.\n","metadata":{}},{"cell_type":"markdown","source":"### 6. Tokenize: return words with position\n-----------------------------------\n***This section explains about tokenizing words.***\n- The input must be unicode\n","metadata":{}},{"cell_type":"code","source":"print('Default Mode\\n')\nresult = jieba.tokenize(u'Ê∞∏ÂíåÊúçË£ÖÈ•∞ÂìÅÊúâÈôêÂÖ¨Âè∏')\nfor tk in result:\n    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:39.356287Z","iopub.execute_input":"2023-09-13T01:00:39.356609Z","iopub.status.idle":"2023-09-13T01:00:39.363986Z","shell.execute_reply.started":"2023-09-13T01:00:39.356581Z","shell.execute_reply":"2023-09-13T01:00:39.362507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Search Mode\\n\")\nresult = jieba.tokenize(u'Ê∞∏ÂíåÊúçË£ÖÈ•∞ÂìÅÊúâÈôêÂÖ¨Âè∏',mode='search')\nfor tk in result:\n    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:39.366017Z","iopub.execute_input":"2023-09-13T01:00:39.366558Z","iopub.status.idle":"2023-09-13T01:00:39.379671Z","shell.execute_reply.started":"2023-09-13T01:00:39.366511Z","shell.execute_reply":"2023-09-13T01:00:39.378266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. ChineseAnalyzer for Whoosh\n-----------------------------------\n***This section explains about tokenizing words.***\n\n`from jieba.analyse import ChineseAnalyzer`<br>\n**Example:** (Copy & Pasted below to see the result)<br> https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py<br>","metadata":{}},{"cell_type":"code","source":"# -*- coding: UTF-8 -*-\nfrom __future__ import unicode_literals\nimport sys,os\nsys.path.append(\"../\")\nfrom whoosh.index import create_in,open_dir\nfrom whoosh.fields import *\nfrom whoosh.qparser import QueryParser\n\nfrom jieba.analyse.analyzer import ChineseAnalyzer\n\nanalyzer = ChineseAnalyzer()\n\nschema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\nif not os.path.exists(\"tmp\"):\n    os.mkdir(\"tmp\")\n\nix = create_in(\"tmp\", schema) # for create new index\n#ix = open_dir(\"tmp\") # for read only\nwriter = ix.writer()\n\nwriter.add_document(\n    title=\"document1\",\n    path=\"/a\",\n    content=\"This is the first document we‚Äôve added!\"\n)\n\nwriter.add_document(\n    title=\"document2\",\n    path=\"/b\",\n    content=\"The second one ‰Ω† ‰∏≠ÊñáÊµãËØï‰∏≠Êñá is even more interesting! ÂêÉÊ∞¥Êûú\"\n)\n\nwriter.add_document(\n    title=\"document3\",\n    path=\"/c\",\n    content=\"‰π∞Ê∞¥ÊûúÁÑ∂ÂêéÊù•‰∏ñÂçöÂõ≠„ÄÇ\"\n)\n\nwriter.add_document(\n    title=\"document4\",\n    path=\"/c\",\n    content=\"Â∑•‰ø°Â§ÑÂ•≥Âπ≤‰∫ãÊØèÊúàÁªèËøá‰∏ãÂ±ûÁßëÂÆ§ÈÉΩË¶Å‰∫≤Âè£‰∫§‰ª£24Âè£‰∫§Êç¢Êú∫Á≠âÊäÄÊúØÊÄßÂô®‰ª∂ÁöÑÂÆâË£ÖÂ∑•‰Ωú\"\n)\n\nwriter.add_document(\n    title=\"document4\",\n    path=\"/c\",\n    content=\"Âí±‰ø©‰∫§Êç¢‰∏Ä‰∏ãÂêß„ÄÇ\"\n)\n\nwriter.commit()\nsearcher = ix.searcher()\nparser = QueryParser(\"content\", schema=ix.schema)\n\nfor keyword in (\"Ê∞¥Êûú‰∏ñÂçöÂõ≠\",\"‰Ω†\",\"first\",\"‰∏≠Êñá\",\"‰∫§Êç¢Êú∫\",\"‰∫§Êç¢\"):\n    print(\"result of \",keyword)\n    q = parser.parse(keyword)\n    results = searcher.search(q)\n    for hit in results:\n        print(hit.highlights(\"content\"))\n    print(\"=\"*10)\n\nfor t in analyzer(\"ÊàëÁöÑÂ•ΩÊúãÂèãÊòØÊùéÊòé;ÊàëÁà±Âåó‰∫¨ÂõΩÂÆâ;IBMÂíåMicrosoft; I have a dream. this is intetesting and interested me a lot\"):\n    print(t.text)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:39.381456Z","iopub.execute_input":"2023-09-13T01:00:39.381866Z","iopub.status.idle":"2023-09-13T01:00:40.017945Z","shell.execute_reply.started":"2023-09-13T01:00:39.381835Z","shell.execute_reply":"2023-09-13T01:00:40.016035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Command Line Interface\n-----------------------------------\n```\n$> python -m jieba --help\nJieba command line interface.\n\npositional arguments:\n  filename              input file\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d [DELIM], --delimiter [DELIM]\n                        use DELIM instead of ' / ' for word delimiter; or a\n                        space if it is used without DELIM\n  -p [DELIM], --pos [DELIM]\n                        enable POS tagging; if DELIM is specified, use DELIM\n                        instead of '_' for POS delimiter\n  -D DICT, --dict DICT  use DICT as dictionary\n  -u USER_DICT, --user-dict USER_DICT\n                        use USER_DICT together with the default dictionary or\n                        DICT (if specified)\n  -a, --cut-all         full pattern cutting (ignored with POS tagging)\n  -n, --no-hmm          don't use the Hidden Markov Model\n  -q, --quiet           don't print loading messages to stderr\n  -V, --version         show program's version number and exit\n\nIf no filename specified, use STDIN instead.\n\n```","metadata":{}},{"cell_type":"markdown","source":"## Initialization\n-----------------------------------\n\nBy default, Jieba don't build the prefix dictionary unless it's necessary. This takes 1-3 seconds, after which it is not initialized again.<br>\nIf you want to initialize Jieba manually, you can call:\n","metadata":{}},{"cell_type":"code","source":"import jieba\njieba.initialize()  # (optional)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.019146Z","iopub.execute_input":"2023-09-13T01:00:40.019546Z","iopub.status.idle":"2023-09-13T01:00:40.027014Z","shell.execute_reply.started":"2023-09-13T01:00:40.019515Z","shell.execute_reply":"2023-09-13T01:00:40.025540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also specify the dictionary (not supported before version 0.28) :","metadata":{}},{"cell_type":"code","source":"jieba.set_dictionary('/kaggle/input/ideogram-phonogram-dataset/dict.txt.big')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.028512Z","iopub.execute_input":"2023-09-13T01:00:40.029105Z","iopub.status.idle":"2023-09-13T01:00:40.044429Z","shell.execute_reply.started":"2023-09-13T01:00:40.029073Z","shell.execute_reply":"2023-09-13T01:00:40.042706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Other Dictionaries\n-----------------------------------\nIt is possible to use your own dictionary with Jieba, and there are also two dictionaries ready for download:<br>\n1. A smaller dictionary for a smaller memory footprint: <br>\nhttps://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small<br>\n\n2. There is also a bigger dictionary that has better support for traditional Chinese (ÁπÅÈ´î):<br>\nhttps://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big<br>\n***You can find both files from [here](https://www.kaggle.com/datasets/jasonheesanglee/ideogram-phonogram-dataset)***<br><br>\n\nBy default, an in-between dictionary is used, called `dict.txt` and included in the distribution.<br>\n\nIn either case, download the file you want, and then call `jieba.set_dictionary('data/dict.txt.big')` or just replace the existing `dict.txt`.\n\n","metadata":{}},{"cell_type":"markdown","source":"## jieba/jieba\n-----------------------------------\nAs I have finished going through README.md, I will start on the original plan.<br>\nBelow is how jieba/jieba directory looks like.<br><br>\nCode explanation done with the help of ChatGPT & BARD\n<img src=\"https://github.com/jasonheesanglee/Ideogram_Phonogram/blob/main/IDEOPHONO/jieba_jieba.png?raw=true\" height=\"100\" />","metadata":{}},{"cell_type":"markdown","source":"### dict.txt\n-----------------------------------\nLet's take a look at dict.txt<br>\nFrom the code output below, we can see that this txt file is composed in a format of `[word]` | `[word frequency]` | `[POS]` as explained in [2.Add a custom dictionary](https://www.kaggle.com/code/jasonheesanglee/ideogram-based-vs-phonogram-based-language?scriptVersionId=142722802&cellId=36).","metadata":{}},{"cell_type":"code","source":"with open(r'/kaggle/input/ideogram-phonogram-dataset/dict.txt') as dict_txt:\n    display(dict_txt.readlines()[0:10])\n#     display(dict_txt.read()[:])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:40.046580Z","iopub.execute_input":"2023-09-13T01:00:40.047216Z","iopub.status.idle":"2023-09-13T01:00:40.196820Z","shell.execute_reply.started":"2023-09-13T01:00:40.047135Z","shell.execute_reply":"2023-09-13T01:00:40.195788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### _compat.py\n-----------------------------------\nI will start with _compat.py as both `main.py` and `init.py` starts by importing this module.<br>\n(Sorry for not including \"___\" in the file name... Hate markdown syntax..)<br>\n\nI will break the module down per `def`.","metadata":{}},{"cell_type":"markdown","source":"### Importing Modules\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport logging\nimport os\nimport sys","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.198427Z","iopub.execute_input":"2023-09-13T01:00:40.199454Z","iopub.status.idle":"2023-09-13T01:00:40.206095Z","shell.execute_reply.started":"2023-09-13T01:00:40.199419Z","shell.execute_reply":"2023-09-13T01:00:40.204504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logging Configurations\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"log_console = logging.StreamHandler(sys.stderr)\ndefault_logger = logging.getLogger(__name__)\ndefault_logger.setLevel(logging.DEBUG)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.198427Z","iopub.execute_input":"2023-09-13T01:00:40.199454Z","iopub.status.idle":"2023-09-13T01:00:40.206095Z","shell.execute_reply.started":"2023-09-13T01:00:40.199419Z","shell.execute_reply":"2023-09-13T01:00:40.204504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `log_console` is created as a Stream Handler that directs log messages to the standard error ('sys.stderr')<br>\n- `default_loger` is a logger object created for the current module.<br>\n- \\_\\_name__ refers to the current module (`_compat.py`)<br>\nIt is configured to log messages with a minimum level of \"DEBUG\"","metadata":{}},{"cell_type":"markdown","source":"### `setLogLevel`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def setLogLevel(log_level):\n#     default_logger.setLevel(log_level)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.213045Z","iopub.execute_input":"2023-09-13T01:00:40.214473Z","iopub.status.idle":"2023-09-13T01:00:40.225907Z","shell.execute_reply.started":"2023-09-13T01:00:40.214417Z","shell.execute_reply":"2023-09-13T01:00:40.224519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function is defined to allow changing the log level of the \"default_logger\".<br>\nBy calling this function with a log level. the logger's level will be set.<br>\nFor example, if the log level is set to `logging.INFO`, the log level will be changed to `INFO`, and the logger will only display messages of INFO level.","metadata":{}},{"cell_type":"code","source":"# check_paddle_install = {'is_paddle_installed': False}\n\n# try:\n#     import pkg_resources\n\n#     get_module_res = lambda *res: pkg_resources.resource_stream(__name__,\n#                                                                 os.path.join(*res))\n# except ImportError:\n#     get_module_res = lambda *res: open(os.path.normpath(os.path.join(\n#         os.getcwd(), os.path.dirname(__file__), *res)), 'rb')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.227542Z","iopub.execute_input":"2023-09-13T01:00:40.227942Z","iopub.status.idle":"2023-09-13T01:00:40.241732Z","shell.execute_reply.started":"2023-09-13T01:00:40.227907Z","shell.execute_reply":"2023-09-13T01:00:40.240257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This part is to check whether the PaddlePaddle library is installed.<br>\n- If `pkg_resources` can be imported, it sets `is_paddle_installed` to True.<br>\n- It uses pkg_resources.resource_stream if available, and if not, it constructs the resource path using os.getcwd() and os.path.dirname(__file__) and opens the resource as a binary file.","metadata":{}},{"cell_type":"markdown","source":"### `enable_paddle`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def enable_paddle():\n#     try:\n#         import paddle\n#     except ImportError:\n#         default_logger.debug(\"Installing paddle-tiny, please wait a minute......\")\n#         os.system(\"pip install paddlepaddle-tiny\")\n#         try:\n#             import paddle\n#         except ImportError:\n#             default_logger.debug(\n#                 \"Import paddle error, please use command to install: pip install paddlepaddle-tiny==1.6.1.\"\n#                 \"Now, back to jieba basic cut......\")\n#     if paddle.__version__ < '1.6.1':\n#         default_logger.debug(\"Find your own paddle version doesn't satisfy the minimum requirement (1.6.1), \"\n#                              \"please install paddle tiny by 'pip install --upgrade paddlepaddle-tiny', \"\n#                              \"or upgrade paddle full version by \"\n#                              \"'pip install --upgrade paddlepaddle (-gpu for GPU version)' \")\n#     else:\n#         try:\n#             import jieba.lac_small.predict as predict\n#             default_logger.debug(\"Paddle enabled successfully......\")\n#             check_paddle_install['is_paddle_installed'] = True\n#         except ImportError:\n#             default_logger.debug(\"Import error, cannot find paddle.fluid and jieba.lac_small.predict module. \"\n#                                  \"Now, back to jieba basic cut......\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.243675Z","iopub.execute_input":"2023-09-13T01:00:40.244887Z","iopub.status.idle":"2023-09-13T01:00:40.256933Z","shell.execute_reply.started":"2023-09-13T01:00:40.244846Z","shell.execute_reply":"2023-09-13T01:00:40.255482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This function begins by importing `paddle`.<br>\n- If the nested import raises an ImportError again, it logs another message to the default logger, indicating that PaddlePaddle couldn't be imported even after the installation and suggests a specific command to install a particular version of PaddlePaddle.\n- The function does not return any values but effectively determines whether PaddlePaddle is available for use in the Jieba library and logs relevant messages.\n- This function handles the installation and availability of the PaddlePaddle library, which may be used by Jieba for certain tasks.<br> If PaddlePaddle is available and of the correct version, it sets the is_paddle_installed flag to True, indicating that PaddlePaddle support is enabled.<br> Otherwise, it falls back to the basic Jieba functionality.","metadata":{}},{"cell_type":"markdown","source":"### `PaddlePaddle`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# PY2 = sys.version_info[0] == 2\n\n# default_encoding = sys.getfilesystemencoding()\n\n# if PY2:\n#     text_type = unicode\n#     string_types = (str, unicode)\n\n#     iterkeys = lambda d: d.iterkeys()\n#     itervalues = lambda d: d.itervalues()\n#     iteritems = lambda d: d.iteritems()\n\n# else:\n#     text_type = str\n#     string_types = (str,)\n#     xrange = range\n\n#     iterkeys = lambda d: iter(d.keys())\n#     itervalues = lambda d: iter(d.values())\n#     iteritems = lambda d: iter(d.items())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.259092Z","iopub.execute_input":"2023-09-13T01:00:40.260212Z","iopub.status.idle":"2023-09-13T01:00:40.278854Z","shell.execute_reply.started":"2023-09-13T01:00:40.260142Z","shell.execute_reply":"2023-09-13T01:00:40.277485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section deals with defining variables and functions based on Python version compatibility (Python 2 and Python 3).\n\n- `PY2 = sys.version_info[0] == 2`:<br>This line determines whether the Python version being used is Python 2.<br>It checks if the major version number (`sys.version_info[0]`) is equal to 2 and assigns the result to the variable PY2.\n- `default_encoding = sys.getfilesystemencoding()`:This line obtains the default encoding used by the file system and assigns it to the variable `default_encoding`.<br>This is often used for encoding and decoding file paths.\n\n***I will pass the first `if` statement as we are using Python 3***\n\n- `text_type = str`: In Python 3, str is used for representing both byte strings and Unicode strings, so it assigns the name text_type to str.\n- `string_types = (str,)`: It defines string_types as a tuple containing only str since there's no need for unicode in Python 3.\n- `xrange = range`: In Python 2, there was a separate xrange function for creating efficient iterators over a range of numbers.<br>In Python 3, the range function provides the same functionality, so it assigns range to xrange.","metadata":{}},{"cell_type":"markdown","source":"### `strdecode`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def strdecode(sentence):\n#     if not isinstance(sentence, text_type):\n#         try:\n#             sentence = sentence.decode('utf-8')\n#         except UnicodeDecodeError:\n#             sentence = sentence.decode('gbk', 'ignore')\n#     return sentence","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.280576Z","iopub.execute_input":"2023-09-13T01:00:40.281662Z","iopub.status.idle":"2023-09-13T01:00:40.294530Z","shell.execute_reply.started":"2023-09-13T01:00:40.281628Z","shell.execute_reply":"2023-09-13T01:00:40.293444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `strdecode` function decodes string to ensure they are in utf-8 format.\n- If it is not utf-8 format, it decodes the sentence with `gbk` encoding.","metadata":{}},{"cell_type":"markdown","source":"### `resolve_filename`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def resolve_filename(f):\n#     try:\n#         return f.name\n#     except AttributeError:\n#         return repr(f)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.296412Z","iopub.execute_input":"2023-09-13T01:00:40.296765Z","iopub.status.idle":"2023-09-13T01:00:40.317412Z","shell.execute_reply.started":"2023-09-13T01:00:40.296725Z","shell.execute_reply":"2023-09-13T01:00:40.315765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `resolve_filename` defines a function named resolve_filename that takes one argument called f, which is expected to be a file object.\n- If the name attribute is not available, it returns a string representation of the file object f using the repr() function.<br>This representation includes information about the object, which can be helpful for debugging or providing more context.","metadata":{}},{"cell_type":"markdown","source":"### __main__.py\n-----------------------------------\nI have hid this part of analysis as it is mostly configurations.","metadata":{}},{"cell_type":"markdown","source":"### Importing modules\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"\"\"\"Jieba command line interface.\"\"\"\n\n# import sys\n# import jieba\n# from argparse import ArgumentParser\n# from ._compat import *","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Argument Parsing\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# parser = ArgumentParser(usage=\"%s -m jieba [options] filename\" % sys.executable, description=\"Jieba command line interface.\", epilog=\"If no filename specified, use STDIN instead.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section sets up the argument parser for the command-line interface of `jieba`.<br>\nIt defines various command-line options that can be used when running the script.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Argument Definitions\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# parser.add_argument(\"-d\", \"--delimiter\", metavar=\"DELIM\", default=' / ',\n#                     nargs='?', const=' ',\n#                     help=\"use DELIM instead of ' / ' for word delimiter; or a space if it is used without DELIM\")\n# parser.add_argument(\"-p\", \"--pos\", metavar=\"DELIM\", nargs='?', const='_',\n#                     help=\"enable POS tagging; if DELIM is specified, use DELIM instead of '_' for POS delimiter\")\n# parser.add_argument(\"-D\", \"--dict\", help=\"use DICT as dictionary\")\n# parser.add_argument(\"-u\", \"--user-dict\",\n#                     help=\"use USER_DICT together with the default dictionary or DICT (if specified)\")\n# parser.add_argument(\"-a\", \"--cut-all\",\n#                     action=\"store_true\", dest=\"cutall\", default=False,\n#                     help=\"full pattern cutting (ignored with POS tagging)\")\n# parser.add_argument(\"-n\", \"--no-hmm\", dest=\"hmm\", action=\"store_false\",\n#                     default=True, help=\"don't use the Hidden Markov Model\")\n# parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", default=False,\n#                     help=\"don't print loading messages to stderr\")\n# parser.add_argument(\"-V\", '--version', action='version',\n#                     version=\"Jieba \" + jieba.__version__)\n# parser.add_argument(\"filename\", nargs='?', help=\"input file\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section add arguments to the parser.\n- `-d` is used to specify a delimiter for word.\n- `-p` is used to enable part of speech tagging.\n- `-D` allows specifying a custom dictionary.\n- `-u` is for a user-defined dictionary.\n- `-a` enables full pattern cutting.\n- `-n` disables the Hidden Markov Model.\n- `-q` makes the script run quietly without loading messages.\n\n***I guess these are the similar terms and functions to*** `!pip install -q ...`.<br>","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Parsing Command-Line Arguments\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# args = parser.parse_args()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section parses the command-line arguments using the previously defined argument parser.<br>\n- The parsed arguments are stored in the `args` variable, which is an object with attributes corresponding to the defined arguments.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Configuration based on Command-Line Arguments.\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# if args.quiet:\n#     jieba.setLogLevel(60)\n\n# if args.pos:\n#     import jieba.posseg\n#     posdelim = args.pos\n#     def cutfunc(sentence, _, HMM=True):\n#         for w, f in jieba.posseg.cut(sentence, HMM):\n#             yield w + posdelim + f\n# else:\n#     cutfunc = jieba.cut","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If the `-q` flag is provided in the command line, it sets the logging level of the jieba library to 60 (which corresponds to the `CRITICAL` log level.<br>\n- This means that loading messages will not be printed to the standard error (stderr)<br>\n\n- If the `-p` flag is provided in the command line, it imports the `jieba.posseg` module and sets up a custom word segmentation function (`cutfunc`) that incorporates POS tags based on the specified delimiter.\n- If the `-p` flag is not provided, it sets `cutfunc` to the default word segmentation function (`jieba.cut`)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Variable Assignments\n-----------------------------------","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# delim = text_type(args.delimiter)\n# cutall = args.cutall\n# hmm = args.hmm\n# fp = open(args.filename, 'r') if args.filename else sys.stdin","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `delim` converts the specified delimiter into appropriate text type. (Either Unicode or byte string)\n- `cutall` stores whether the `-a` flag was provided.\n- `hmm` stores whether `-n` flag was provided.\n- `fp` opens the input file specified in the command line `arg.filename` or `sys.stdin` if filename is not provided.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### `jieba` Configuration\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# if args.dict:\n#     jieba.initialize(args.dict)\n# else:\n#     jieba.initialize()\n# if args.user_dict:\n#     jieba.load_userdict(args.user_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If the `-D` flag is provided, it initializes jieba with the specified dictionary.<br>Otherwise, it uses the default dictionary.\n- If the `-u` flag is provided, it loads the specified user dictionary.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Processing and Output\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# ln = fp.readline()\n# while ln:\n#     l = ln.rstrip('\\r\\n')\n#     result = delim.join(cutfunc(ln.rstrip('\\r\\n'), cutall, hmm))\n#     if PY2:\n#         result = result.encode(default_encoding)\n#     print(result)\n#     ln = fp.readline()\n\n# fp.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section reads lines from the input file (or stdin if no filename is provided) using fp.readline().\n- It applies the word segmentation function (cutfunc) to each line, joining the resulting tokens with the specified delimiter.\n- If the Python version is 2.x (PY2 is True), it encodes the result using the default encoding.\n- It prints the segmented and possibly encoded text to the standard output.\n- This process continues until there are no more lines to read.\n- Finally, it closes the input file (if opened).","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### __init__.py\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# $Below\\ In\\ Progress$\n","metadata":{}},{"cell_type":"markdown","source":"# Combining Columns","metadata":{}},{"cell_type":"code","source":"cn_new = pd.DataFrame()\nen_new = pd.DataFrame()\ncn_new['input'] = \"Headline: \" + cn_example['headline'] + \"; Content: \" + cn_example['content']\nen_new['input'] = \"Headline: \" + en_example['headline'] + \"; Content: \" + en_example['content']","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.325382Z","iopub.status.idle":"2023-09-12T12:30:44.325741Z","shell.execute_reply.started":"2023-09-12T12:30:44.325562Z","shell.execute_reply":"2023-09-12T12:30:44.325578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cn_new[cn_new['input'].isna()==True]\ncn_new = cn_new.dropna()\ncn_new[cn_new['input'].isna()==True]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.326823Z","iopub.status.idle":"2023-09-12T12:30:44.327182Z","shell.execute_reply.started":"2023-09-12T12:30:44.327007Z","shell.execute_reply":"2023-09-12T12:30:44.327030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_new[en_new['input'].isna()==True]\nen_new = en_new.dropna()\nen_new[en_new['input'].isna()==True]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.328306Z","iopub.status.idle":"2023-09-12T12:30:44.328657Z","shell.execute_reply.started":"2023-09-12T12:30:44.328489Z","shell.execute_reply":"2023-09-12T12:30:44.328506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(cn_new.head(5))\ndisplay(en_new.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.330013Z","iopub.status.idle":"2023-09-12T12:30:44.330368Z","shell.execute_reply.started":"2023-09-12T12:30:44.330200Z","shell.execute_reply":"2023-09-12T12:30:44.330216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization\nTokenization is the process of breaking down a text into smaller units, which are typically words or subwords.<br>These smaller units are called tokens.<br><br>\nIn English, tokenization usually involves splitting text into words based on spaces, punctuation, or other delimiters.<br>\nFor example, the sentence \"I love ice cream\" would be tokenized into the tokens: [\"I\", \"love\", \"ice\", \"cream\"].<br><br>\nIn languages like Chinese, tokenization can be more complex since <br>Tokenization might involve segmenting text into characters or meaningful subword units.\n<br>\n\n### Trying Out SentencePiece\nGot the [Colab Doc](https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=SUcAbKnRVAv6), but not sure how to use it yet.<br>\nAs always, [abhishek](https://www.kaggle.com/abhishek)'s [notebook](https://www.kaggle.com/code/abhishek/sentencepiece-tokenizer-with-offsets/notebook) helped me a lot on SentencePiece implementation.","metadata":{}},{"cell_type":"code","source":"class SentencePieceTokenizer:\n    '''\n    from Abhishek Thakur's notebook\n    https://www.kaggle.com/code/abhishek/sentencepiece-tokenizer-with-offsets\n    '''\n    def __init__(self, model_path):\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load(model_path +'.model')\n        \n    def encode(self, sentence):\n        spt = sentencepiece_pb2.SentencePieceText()\n        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n        offsets = []\n        tokens = []\n        for piece in spt.pieces:\n            tokens.append(piece.id)\n            offsets.append((piece.begin, piece.end))\n        return tokens, offsets","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.331721Z","iopub.status.idle":"2023-09-12T12:30:44.332117Z","shell.execute_reply.started":"2023-09-12T12:30:44.331898Z","shell.execute_reply":"2023-09-12T12:30:44.331913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_tok = AutoTokenizer.from_pretrained('/kaggle/input/debertav3base')\ndeberta_tok.save_pretrained('/kaggle/working/deberta_tok/')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.333141Z","iopub.status.idle":"2023-09-12T12:30:44.333475Z","shell.execute_reply.started":"2023-09-12T12:30:44.333313Z","shell.execute_reply":"2023-09-12T12:30:44.333329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spt = SentencePieceTokenizer('/kaggle/input/debertav3base/spm')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.334536Z","iopub.status.idle":"2023-09-12T12:30:44.334876Z","shell.execute_reply.started":"2023-09-12T12:30:44.334709Z","shell.execute_reply":"2023-09-12T12:30:44.334725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\nHere I have tried on encoding different words, languages and tokens.<br>\nLet's see if it works well!<br>","metadata":{}},{"cell_type":"markdown","source":"#### Tokens\nIt is fun to see how these tokens are treated *similarly*","metadata":{}},{"cell_type":"code","source":"print(f\"[MASK] encoded into \\t{spt.encode('[MASK]')}\")\nprint(f\"[CLS] encoded into \\t{spt.encode('[CLS]')}\")\nprint(f\"[EOS] encoded into \\t{spt.encode('[EOS]')}\")\nprint(f\"[UNK] encoded into \\t{spt.encode('[UNK]')}\")\nprint(f\"[SEP] encoded into \\t{spt.encode('[SEP]')}\")\nprint(f\"[SPECIAL] encoded into \\t{spt.encode('[SPECIAL]')}\")\nprint()\nprint(f\"MASK encoded into \\t{spt.encode('MASK')}\")\nprint(f\"CLS encoded into \\t{spt.encode('CLS')}\")\nprint(f\"EOS encoded into \\t{spt.encode('EOS')}\")\nprint(f\"UNK encoded into \\t{spt.encode('UNK')}\")\nprint(f\"SEP encoded into \\t{spt.encode('SEP')}\")\nprint(f\"SPECIAL encoded into \\t{spt.encode('SPECIAL')}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.335803Z","iopub.status.idle":"2023-09-12T12:30:44.336184Z","shell.execute_reply.started":"2023-09-12T12:30:44.335993Z","shell.execute_reply":"2023-09-12T12:30:44.336013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"[MASK] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[MASK]', add_special_tokens=False)}\")\nprint(f\"\\n[CLS] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[CLS]', add_special_tokens=False)}\")\nprint(f\"\\n[EOS] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[EOS]', add_special_tokens=False)}\")\nprint(f\"\\n[UNK] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[UNK]', add_special_tokens=False)}\")\nprint(f\"\\n[SEP] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[SEP]', add_special_tokens=False)}\")\nprint(f\"\\n[SPECIAL] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[SPECIAL]', add_special_tokens=False)}\")\nprint()\nprint(f\"\\nMASK encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('MASK', add_special_tokens=False)}\")\nprint(f\"\\nCLS encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('CLS', add_special_tokens=False)}\")\nprint(f\"\\nEOS encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('EOS', add_special_tokens=False)}\")\nprint(f\"\\nUNK encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('UNK', add_special_tokens=False)}\")\nprint(f\"\\nSEP encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('SEP', add_special_tokens=False)}\")\nprint(f\"\\nSPECIAL encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('SPECIAL', add_special_tokens=False)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.337390Z","iopub.status.idle":"2023-09-12T12:30:44.337740Z","shell.execute_reply.started":"2023-09-12T12:30:44.337567Z","shell.execute_reply":"2023-09-12T12:30:44.337583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### English words\nWe can see that these sample English words are encoded properly *(maybe?)*","metadata":{}},{"cell_type":"code","source":"display(spt.encode(en_new.input[0][:10]))\ndisplay(spt.encode(en_new.input[0][:15]))\ndisplay(spt.encode(en_new.input[0][:30]))\ndisplay(spt.encode(en_new.input[0][:50]))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.338845Z","iopub.status.idle":"2023-09-12T12:30:44.339203Z","shell.execute_reply.started":"2023-09-12T12:30:44.339035Z","shell.execute_reply":"2023-09-12T12:30:44.339052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(deberta_tok(en_new.input[0][:10], add_special_tokens=False))\ndisplay(deberta_tok(en_new.input[0][:15], add_special_tokens=False))\ndisplay(deberta_tok(en_new.input[0][:30], add_special_tokens=False))\ndisplay(deberta_tok(en_new.input[0][:50], add_special_tokens=False))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.340169Z","iopub.status.idle":"2023-09-12T12:30:44.340509Z","shell.execute_reply.started":"2023-09-12T12:30:44.340340Z","shell.execute_reply":"2023-09-12T12:30:44.340357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chinese words\nNow the problem begins, the last digit of the encoded tensors different.<br> *(which I think is the bytes taken)*<br>\nWhich implies that this doesn't work at all for Chinese words.<br><br>\n**edit**<br>\nWait what...?<br>\nLast time I checked, there were no differences between ‰Ω† and ÊÇ®.<br>\nBut when I check it now, there is a difference...<br>\nIt seems like deberta is also working well for Chinese characters.","metadata":{}},{"cell_type":"code","source":"display(spt.encode(cn_new.input[0][:10]))\ndisplay(spt.encode(cn_new.input[0][:15]))\ndisplay(spt.encode(cn_new.input[0][:30]))\ndisplay(spt.encode(cn_new.input[0][:40]))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.341445Z","iopub.status.idle":"2023-09-12T12:30:44.341776Z","shell.execute_reply.started":"2023-09-12T12:30:44.341613Z","shell.execute_reply":"2023-09-12T12:30:44.341628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(deberta_tok(cn_new.input[0][:10], add_special_tokens=False))\ndisplay(deberta_tok(cn_new.input[0][:15], add_special_tokens=False))\ndisplay(deberta_tok(cn_new.input[0][:30], add_special_tokens=False))\ndisplay(deberta_tok(cn_new.input[0][:40], add_special_tokens=False))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.342731Z","iopub.status.idle":"2023-09-12T12:30:44.343083Z","shell.execute_reply.started":"2023-09-12T12:30:44.342893Z","shell.execute_reply":"2023-09-12T12:30:44.342908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So I tried... and this works!!<br>\n**edit** I will leave below as it is.<br>","metadata":{}},{"cell_type":"code","source":"cn_spt = SentencePieceTokenizer('/kaggle/input/sentencepiece-chinese-bpe/chinese/chinese')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.343843Z","iopub.status.idle":"2023-09-12T12:30:44.344236Z","shell.execute_reply.started":"2023-09-12T12:30:44.344054Z","shell.execute_reply":"2023-09-12T12:30:44.344073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(cn_spt.encode(cn_new.input[0][:10]))\ndisplay(cn_spt.encode(cn_new.input[0][:15]))\ndisplay(cn_spt.encode(cn_new.input[0][:30]))\ndisplay(cn_spt.encode(cn_new.input[0][:40]))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.346566Z","iopub.status.idle":"2023-09-12T12:30:44.347131Z","shell.execute_reply.started":"2023-09-12T12:30:44.346827Z","shell.execute_reply":"2023-09-12T12:30:44.346852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Korean words\nSame here, I am pretty... no very sure that this doesn't work on foreign languages.<br><br>\n**edit**<br>\nOops.. again, this seems like it works well...!","metadata":{}},{"cell_type":"code","source":"display(spt.encode('ÎÇò'))\ndisplay(spt.encode('Ï†ú'))\nprint()\ndisplay(spt.encode('ÏïàÎÖï'))\ndisplay(spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî'))\nprint()\ndisplay(spt.encode('Ïù¥Î¶Ñ'))\ndisplay(spt.encode('Ïù¥Î¶ÑÏùÄ'))\nprint()\ndisplay(spt.encode('Ïù¥Ìù¨ÏÉÅ'))\ndisplay(spt.encode('Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§'))\nprint()\ndisplay(spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî Ï†ú Ïù¥Î¶ÑÏùÄ'))\ndisplay(spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî Ï†ú Ïù¥Î¶ÑÏùÄ Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§'))\ndisplay(spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî. Ï†ú Ïù¥Î¶ÑÏùÄ Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§. ÌôçÎåÄÏóêÏÑú Í≥µÎ∂ÄÎ•º ÌïòÍ≥† ÏûàÏäµÎãàÎã§.'))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.348265Z","iopub.status.idle":"2023-09-12T12:30:44.348768Z","shell.execute_reply.started":"2023-09-12T12:30:44.348511Z","shell.execute_reply":"2023-09-12T12:30:44.348534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(deberta_tok('ÎÇò', add_special_tokens=False))\ndisplay(deberta_tok('Ï†ú', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('ÏïàÎÖï', add_special_tokens=False))\ndisplay(deberta_tok('ÏïàÎÖïÌïòÏÑ∏Ïöî', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('Ïù¥Î¶Ñ', add_special_tokens=False))\ndisplay(deberta_tok('Ïù¥Î¶ÑÏùÄ', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('Ïù¥Ìù¨ÏÉÅ', add_special_tokens=False))\ndisplay(deberta_tok('Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('ÏïàÎÖïÌïòÏÑ∏Ïöî Ï†ú Ïù¥Î¶ÑÏùÄ', add_special_tokens=False))\ndisplay(deberta_tok('ÏïàÎÖïÌïòÏÑ∏Ïöî Ï†ú Ïù¥Î¶ÑÏùÄ Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§', add_special_tokens=False))\ndisplay(deberta_tok('ÏïàÎÖïÌïòÏÑ∏Ïöî. Ï†ú Ïù¥Î¶ÑÏùÄ Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§. ÌôçÎåÄÏóêÏÑú Í≥µÎ∂ÄÎ•º ÌïòÍ≥† ÏûàÏäµÎãàÎã§.', add_special_tokens=False))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.350363Z","iopub.status.idle":"2023-09-12T12:30:44.350864Z","shell.execute_reply.started":"2023-09-12T12:30:44.350607Z","shell.execute_reply":"2023-09-12T12:30:44.350631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, I tried as below!<br>\nIt seems like it is working properly! :)<br><br>\n\n**edit**<br>\nI will leave below as it is.<br>","metadata":{}},{"cell_type":"code","source":"ko_spt = SentencePieceTokenizer('/kaggle/input/airc-keti-ke-t5/vocab/sentencepiece_v2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.352007Z","iopub.status.idle":"2023-09-12T12:30:44.352537Z","shell.execute_reply.started":"2023-09-12T12:30:44.352284Z","shell.execute_reply":"2023-09-12T12:30:44.352308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(ko_spt.encode('ÎÇò'))\ndisplay(ko_spt.encode('Ï†ú'))\nprint()\ndisplay(ko_spt.encode('ÏïàÎÖï'))\ndisplay(ko_spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî'))\nprint()\ndisplay(ko_spt.encode('Ïù¥Î¶Ñ'))\ndisplay(ko_spt.encode('Ïù¥Î¶ÑÏùÄ'))\nprint()\ndisplay(ko_spt.encode('Ïù¥Ìù¨ÏÉÅ'))\ndisplay(ko_spt.encode('Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§'))\nprint()\ndisplay(ko_spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî Ï†ú Ïù¥Î¶ÑÏùÄ'))\ndisplay(ko_spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî Ï†ú Ïù¥Î¶ÑÏùÄ Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§'))\ndisplay(ko_spt.encode('ÏïàÎÖïÌïòÏÑ∏Ïöî. Ï†ú Ïù¥Î¶ÑÏùÄ Ïù¥Ìù¨ÏÉÅÏûÖÎãàÎã§. ÌôçÎåÄÏóêÏÑú Í≥µÎ∂ÄÎ•º ÌïòÍ≥† ÏûàÏäµÎãàÎã§.'))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.354079Z","iopub.status.idle":"2023-09-12T12:30:44.354574Z","shell.execute_reply.started":"2023-09-12T12:30:44.354321Z","shell.execute_reply":"2023-09-12T12:30:44.354344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}