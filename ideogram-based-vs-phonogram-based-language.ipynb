{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🀄 Ideogram-based vs. Phonogram-based Language\n#### Jason Heesang Lee","metadata":{}},{"cell_type":"code","source":"!pip install -q whoosh","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:12.991754Z","iopub.execute_input":"2023-09-13T01:00:12.992159Z","iopub.status.idle":"2023-09-13T01:00:30.441584Z","shell.execute_reply.started":"2023-09-13T01:00:12.992129Z","shell.execute_reply":"2023-09-13T01:00:30.440361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Disclaimer:* Initially, this project was just a quick review to fulfill my curiosity.<br>\nBut as I was developing this notebook, somehow it became a large-sized project..<br>\nI will try to finish this notebook by ***October 2023***.<br>\n\n------------------------------------------------------------------\n<br>\n\n***There*** are more than **7.8 billion** people in the world and with more than **7,000 languages**.<br><br>\nIn a greater perspective, there are two types of languages: Ideogram-Based Language and Phonogram-Based Language.<br>\nPhonogram-based Languages are languages that are developed on phonemes (speech sound) or a combination of phonemes.<br>\nLatin alphabets and Korean (Hangul) are examples.<br>\n<br>\nIdeogram Based Languages are the languages that are developed on symbols of writing systems.\nChinese, Egyptian Hieroglyph and Sumero-Akkadian Cuneiform are examples.<br> [Source: Wikipedia](https://en.wikipedia.org/wiki/Ideogram)<br>\n<br>\nAs I am fluent in Korean, English, and Chinese, I was suddenly curious about the possible differences in Natural Language Processing (NLP) techniques dealing with these two types of languages.<br> [Source: Wikipedia](https://en.wikipedia.org/wiki/Phonogram_(linguistics))<br>\n<br>\nIn a brief thoughts, I believe it is easier to process Ideogram Based Languages than Phonogram Based Languages.<br>\n<br>\nIt is due to the characteristics of the Ideogram Based Languages.\n<br>\nTaking Chinese (which I am familiar with) as an example, each character represents a certain definition. Each character (or an alphabet) in Phonogram Based Languages such as English and Hangul, often needs other characters to contain a definition.<br>\n<br>\n**`Hypothesis`** : Ideogram-based Languages might not need special Tokenizations or Embeddings for Natural Language Processing.<br>\n<br>\nI tried to ask and discuss with the lecturers here at Year-Dream School (Data Science Bootcamp) regarding this topic.<br>\nI only had a meaningful discussion with [@Yongdam Kim](https://www.kaggle.com/emphymachine) as he and some of his friend has some (not a lot, as per he claims) experience in this field.<br>\nHe mentioned that Natural Language Processing can be easier for Ideogram-based languages, as each character in this language contains meaning, which already could be similar to embedding.<br>\n<br>\nAs I want to further research into this topic, I had to first ask ChatGPT and Google Bard to fulfill my curiosity.<br>\n<br>\n***My query was as below.***<br>\n\n> *I was recently wondering that NLP process could be different between Phonogram based languages like English, and Ideogram based language like Chinese.<br>\nLike Tokenization, Embedding, Vectorization, Lemmatization, Stemming, etc.<br>\nCould you tell me the main differences in process of Natural Language Processing?*\n>\n\nBelow are the responses from the LLMs (redirected to my Notion page)<br><br>\n**`ChatGPT`**<br>\n[GPT - NLP Phonogram Ideogram.pdf](https://www.notion.so/jason-heesang-lee/Ideogram-Based-Language-vs-Phonogram-Based-Language-6ba064e320e2413aaba60f6aba5e6e19?pvs=4#b378a20a10ca4580b4442a5a4486b87f)<br>\n<br>\n**`Google Bard`**<br>\n[Bard - NLP Phonogram Ideogram.pdf](https://www.notion.so/jason-heesang-lee/Ideogram-Based-Language-vs-Phonogram-Based-Language-6ba064e320e2413aaba60f6aba5e6e19?pvs=4#11e5c642f4394e1c82311677d4ea1268)<br>\n<br>\nThere were some points that were interesting.<br>\n<br>\nFirst, both GPT and Bard told me that the Tokenization process might be harder on Ideogram-based Languages.<br>\nTokenization is the process of decomposing a sentence into words.<br>\nAs each word is represented in a way as a sequence of characters, it would be easier for the tokenizing process.<br>","metadata":{}},{"cell_type":"markdown","source":"My plan is to open up each key modules and figure out how they work.<br>\nFor Jieba, I will try to understand how this module is able to perform such segmentation.<br>\nAlso for DeBERTa Tokenizer or AutoTokenizer (I need to find out which module makes the difference), I want to know the inner logic that processes English and Chinese with the same lines of code.<br>","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/sentencepiece-pb2/\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")\nimport re\nimport nltk\nimport jieba\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport sentencepiece_pb2\nimport sentencepiece as spm\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:30.443612Z","iopub.execute_input":"2023-09-13T01:00:30.444010Z","iopub.status.idle":"2023-09-13T01:00:34.977715Z","shell.execute_reply.started":"2023-09-13T01:00:30.443976Z","shell.execute_reply":"2023-09-13T01:00:34.976801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\nI have brought [Chinese Daily News](https://www.kaggle.com/datasets/noxmoon/chinese-official-daily-news-since-2016) by [@noxmoon](https://www.kaggle.com/noxmoon) & True news from [Fake and Real News](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset) by [@clmentbisaillon](https://www.kaggle.com/clmentbisaillon) to compare the process.<br>","metadata":{}},{"cell_type":"code","source":"cn_df = pd.read_csv('/kaggle/input/chinese-official-daily-news-since-2016/chinese_news.csv', encoding='utf-8')\ndisplay(cn_df.head(5))\nprint()\nen_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\ndisplay(en_df.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:34.978977Z","iopub.execute_input":"2023-09-13T01:00:34.980478Z","iopub.status.idle":"2023-09-13T01:00:36.885229Z","shell.execute_reply.started":"2023-09-13T01:00:34.980431Z","shell.execute_reply":"2023-09-13T01:00:36.884023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking DataFrame Information","metadata":{}},{"cell_type":"code","source":"print(f\"cn_df.info() :\\n{cn_df.info()}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.887992Z","iopub.execute_input":"2023-09-13T01:00:36.888503Z","iopub.status.idle":"2023-09-13T01:00:36.931913Z","shell.execute_reply.started":"2023-09-13T01:00:36.888470Z","shell.execute_reply":"2023-09-13T01:00:36.930416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"en_df.info() :\\n{en_df.info()}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.933651Z","iopub.execute_input":"2023-09-13T01:00:36.935459Z","iopub.status.idle":"2023-09-13T01:00:36.961864Z","shell.execute_reply.started":"2023-09-13T01:00:36.935415Z","shell.execute_reply":"2023-09-13T01:00:36.960481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dropping unnecessary columns\nI will drop date & tag columns from each DataFrame.<br>\nAnd matched the column names.","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"cn_df = cn_df.drop(columns=['date', 'tag'])\nen_df = en_df.drop(columns=['date', 'subject'])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.963785Z","iopub.execute_input":"2023-09-13T01:00:36.964121Z","iopub.status.idle":"2023-09-13T01:00:36.980463Z","shell.execute_reply.started":"2023-09-13T01:00:36.964091Z","shell.execute_reply":"2023-09-13T01:00:36.979531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"list(cn_df.columns) :\\n{list(cn_df.columns)}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.982277Z","iopub.execute_input":"2023-09-13T01:00:36.983038Z","iopub.status.idle":"2023-09-13T01:00:36.989771Z","shell.execute_reply.started":"2023-09-13T01:00:36.982992Z","shell.execute_reply":"2023-09-13T01:00:36.988507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_df = en_df.rename(columns={'title':'headline', 'text':'content'})\nprint(f\"list(en_df.columns) :\\n{list(en_df.columns)}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:36.991466Z","iopub.execute_input":"2023-09-13T01:00:36.991917Z","iopub.status.idle":"2023-09-13T01:00:37.013632Z","shell.execute_reply.started":"2023-09-13T01:00:36.991875Z","shell.execute_reply":"2023-09-13T01:00:37.010073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Most of NLP Technique retrieved from [@jhoward](https://www.kaggle.com/jhoward)'s notebook\n***[Getting started with NLP for absolute beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)***","metadata":{}},{"cell_type":"code","source":"cn_example = pd.DataFrame(cn_df.iloc[0]).T\nprint(f\"cn_example :\\n{cn_example}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:37.015378Z","iopub.execute_input":"2023-09-13T01:00:37.016439Z","iopub.status.idle":"2023-09-13T01:00:37.033530Z","shell.execute_reply.started":"2023-09-13T01:00:37.016403Z","shell.execute_reply":"2023-09-13T01:00:37.032241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_example = pd.DataFrame(en_df.iloc[0]).T\nprint(f\"en_example :\\n{en_example}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:37.038027Z","iopub.execute_input":"2023-09-13T01:00:37.038946Z","iopub.status.idle":"2023-09-13T01:00:37.049388Z","shell.execute_reply.started":"2023-09-13T01:00:37.038900Z","shell.execute_reply":"2023-09-13T01:00:37.047949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CN Text Preprocessing\n##### CN Definition retrieved from [Baidu Wenku](https://wenku.baidu.com/view/039d6d4e551252d380eb6294dd88d0d233d43cc8.html?_wkts_=1693548615550&bdQuery=%E4%B8%AD%E6%96%87+%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80+%E5%89%8D%E5%A4%84%E7%90%86)","metadata":{}},{"cell_type":"code","source":"# stopwords = [k.strip() for k in open('/kaggle/input/english-and-chinese-stopwords/stopwords.txt', encoding='utf8').readlines() if k.strip() != '']\n\ndef find_chinese(text):\n    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n    chinese_txt = re.sub(pattern,'',text)\n    return str(chinese_txt)\n\ndef cut_words(text):\n    jieba_txt = ' '.join(jieba.cut(find_chinese(text), cut_all=False))\n    return jieba_txt\n\n# def seg_sentence(text_list):\n#     seg_text = [word for word in text_list if word not in stopwords]\n#     return seg_text","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:37.051028Z","iopub.execute_input":"2023-09-13T01:00:37.052382Z","iopub.status.idle":"2023-09-13T01:00:37.064088Z","shell.execute_reply.started":"2023-09-13T01:00:37.052330Z","shell.execute_reply":"2023-09-13T01:00:37.062629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile cn_text.txt\n ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:37.066147Z","iopub.execute_input":"2023-09-13T01:00:37.067469Z","iopub.status.idle":"2023-09-13T01:00:37.080729Z","shell.execute_reply.started":"2023-09-13T01:00:37.067420Z","shell.execute_reply":"2023-09-13T01:00:37.079737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cn_text_file = open('/kaggle/working/cn_text.txt', 'w')\ncn_text = ''\nfor column in cn_example.columns:\n    temp = []\n    for row in tqdm(range(cn_example.shape[0])):\n        text = cn_example.iloc[row][column]\n        text = cut_words(text)\n        \n        temp.append(text)\n        cn_text = cn_text + '; ' + text\n#     text = seg_sentence(temp)\n    cn_example[column] = pd.Series(temp)\n\n#     cn_example[column] = pd.Series(seg_sentence(temp))\ncn_text_file.write(cn_text)\ndisplay(cn_example.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:37.082214Z","iopub.execute_input":"2023-09-13T01:00:37.082754Z","iopub.status.idle":"2023-09-13T01:00:38.531169Z","shell.execute_reply.started":"2023-09-13T01:00:37.082721Z","shell.execute_reply":"2023-09-13T01:00:38.529765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EN Text Preproecessing\nI will drop the names of the news companies.","metadata":{}},{"cell_type":"code","source":"%%writefile en_text.txt\n ","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:38.532917Z","iopub.execute_input":"2023-09-13T01:00:38.533438Z","iopub.status.idle":"2023-09-13T01:00:38.541029Z","shell.execute_reply.started":"2023-09-13T01:00:38.533391Z","shell.execute_reply":"2023-09-13T01:00:38.539521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = []\nen_text_file = open('/kaggle/working/en_text.txt', 'w')\n\nen_text = ''\nfor row in tqdm(range(en_example.shape[0])):\n    text_h = en_df.headline[row]\n    text = \" \".join(en_example.content[row].split(' - ')[1:])\n    en_text = en_text + '; ' + text_h\n    en_text = en_text + \"; \" + text\n    temp.append(text)\n\nen_text_file.write(en_text)\nen_example.content = pd.Series(temp)\ndisplay(en_example.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.543285Z","iopub.execute_input":"2023-09-13T01:00:38.543772Z","iopub.status.idle":"2023-09-13T01:00:38.570055Z","shell.execute_reply.started":"2023-09-13T01:00:38.543726Z","shell.execute_reply":"2023-09-13T01:00:38.568415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking text files","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/cn_text.txt') as cn_text_file:\n    print(cn_text_file.read())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.572310Z","iopub.execute_input":"2023-09-13T01:00:38.572820Z","iopub.status.idle":"2023-09-13T01:00:38.581140Z","shell.execute_reply.started":"2023-09-13T01:00:38.572773Z","shell.execute_reply":"2023-09-13T01:00:38.579581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/en_text.txt') as en_text_file:\n    print(en_text_file.read())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.583006Z","iopub.execute_input":"2023-09-13T01:00:38.583507Z","iopub.status.idle":"2023-09-13T01:00:38.598959Z","shell.execute_reply.started":"2023-09-13T01:00:38.583461Z","shell.execute_reply":"2023-09-13T01:00:38.597452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Jieba\n***I guess this is where I have to examine the [Jieba Github](https://github.com/fxsjy/jieba)...!***\n\n**This is how the repository looks like.**<br>\n<img src=\"https://github.com/jasonheesanglee/Ideogram_Phonogram/blob/main/IDEOPHONO/jieba_main.png?raw=true\" height=\"100\" /><br>\nThere are 3 different directories - extra_dict, jieba, test, and some config files.<br>\nLet's first look into README.md to grasp the concept of what this module is in the end.<br>\nI brought English version of README.<br>\n(The content is identical with the Chinese version.)","metadata":{}},{"cell_type":"markdown","source":"## jieba\n-----------------------------------\n\n*Jieba (Chinese for \"to stutter\") Chinese text segmentation: built to be the best Python Chinese word segmentation module.*<br>\n***This is the explanation of what jieba is***<br>","metadata":{}},{"cell_type":"markdown","source":"### Features\n-----------------------------------\n- Support three types of segmentation mode:\n1. Accurate Mode attempts to cut the sentence into the most accurate segmentations, which is suitable for text analysis.\n2. Full Mode gets all the possible words from the sentence. Fast but not accurate.\n3. Search Engine Mode, based on the Accurate Mode, attempts to cut long words into several short words, which can raise the recall rate. Suitable for search engines.\n- Supports Traditional Chinese\n- Supports customized dictionaries\n- MIT License\n\n***There are 3 different segmentation modes, and the usage of each mode differs from the purpose of the usage.***","metadata":{}},{"cell_type":"markdown","source":"### Online demo \n-----------------------------------\n\n[http://jiebademo.ap01.aws.af.cm/](http://jiebademo.ap01.aws.af.cm/)<br>\n***This online demo is not working anymore (404 Error)***","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Usage\n-----------------------------------\n\n- Fully automatic installation: `easy_install jieba` or `pip install jieba`<br>\n- Semi-automatic installation: Download [http://pypi.python.org/pypi/jieba/](https://pypi.org/project/jieba/) , run `python setup.py install` after extracting.<br>\n- Manual installation: place the `jieba` directory in the current directory or python `site-packages` directory.<br>\n- `import jieba`.<br>\n\n***This section explains how to import the module***","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Algorithm\n-----------------------------------\n\n- Based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations.\n- Use dynamic programming to find the most probable combination based on the word frequency.\n- For unknown words, a HMM-based model is used with the Viterbi algorithm.\n\n***Wait, there are so many terms I have no clue about.<br>What is Directed Acyclic Graph? <br>What is HMM-based model? <br>What is Viterbi algorithm?***\n\n#### Directed Acyclic Graph (DAG)\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Tred-G.svg/1920px-Tred-G.svg.png\" width=200 />\n\nBased on [Wikipedia](https://en.wikipedia.org/wiki/Directed_acyclic_graph), Directed Acyclic Graph is a \n>*Directed graph with no directed cycles. A directed graph is a DAG if and only if it can be [topologically ordered](https://en.wikipedia.org/wiki/Topological_order), by arranging the vertices as a linear ordering that is consistent with all edge directions.* <br>\n\nWhat? still no clue yet, let's move on to the definitions.<br>\n\n> *A graph is formed by vertices and by edges connecting pairs of vertices, where the vertices can be any kind of object that is connected in pairs by edges. In the case of a directed graph, each edge has an orientation, from one vertex to another vertex. A path in a directed graph is a sequence of edges having the property that the ending vertex of each edge in the sequence is the same as the starting vertex of the next edge in the sequence; a path forms a cycle if the starting vertex of its first edge equals the ending vertex of its last edge. A directed acyclic graph is a directed graph that has no cycles.*<br><br>\n\n**TL;DR** (I shouldn't though)\n\nInstead of learning it from Wikipedia, I searched Google a bit more and completely understood the concept from [StackExchange](https://math.stackexchange.com/questions/3782987/difference-between-oriented-graph-and-directed-acyclic-graphs-dag#:~:text=Basically%20directed%20graphs%20can%20have,two%20vertices%20A%20and%20B.&text=In%20mathematics%2C%20particularly%20graph%20theory,graph%20with%20no%20directed%20cycles.)<br>\nPlease correct me if I am wrong;<br>\nBasically DAG is a graph of number of vertices connected by edges (with direction), and this edge doesn't go back but only go forth.<br> Which makes this graph a graph with direction, but not circulating.<br>\n***OH*** That is why its name is **Directed** **A**cyclic Graph!!\n\n#### HMM-based model\nBased on this [article](https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4) by [Mehul Gupta](https://medium.com/@mehulgupta_7991), to understand the conecept of Hidden Markov Model (HMM)-based model, we need to understand what ***Markov Chain*** is.<br>\nIt gave a simple definition of Markov chain and I completely got it!\n> *A Markov chain is a model that tells us something about the probabilities of sequences of random states/variables. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state.*\n\nBelow is an example the writer gave, and I believe this is just a perfect example.\n> *A Markov Chain model based on Weather might have Hot, Cool, and Rainy as its states & to predict tomorrow’s weather you could examine today’s weather but yesterday’s weather isn’t significant in the prediction.*<br>\n\nBelow are specified all the components of Markov Chains.\n<img src=\"https://miro.medium.com/v2/resize:fit:1104/format:webp/1*tI5HGo_cFTxgcxiEDHQMOw.png\" height=100 />\n\nMoving on to ***HMM-based model***.<br>\n> *Sometimes, what we want to predict is a sequence of states that aren’t directly observable in the environment. Though we are given another sequence of states that are observable in the environment, these hidden states have some dependence on the observable states.*\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXjrDa28pUnGmI0ehjhR8A.png\" height=100 /><br>\n> *In the above HMM, we are given Walk, Shop & Clean as observable states. But we are more interested in tracing the sequence of the hidden states that will be followed which are Rainy & Sunny.*<br>\n\n***As per my understanding, simply saying, this is an advanced step after Markov Chain.<br>\nAccording to the provided image, actions are the subjects that we are predicting, and the original factors of Markov Chain example (weather conditions), are the hidden layer (state) that influences the prediction of the action.***<br>\n\nHidden Markov Model is needed for Part of Speech Tagging (Categories of words; verbs, nouns, actions, expresssions and so on).<br>\n> If you notice closely, we can have the words in a sentence as Observable States (given to us in the data) but their POS Tags as Hidden states and hence we use HMM for estimating POS tags. It must be noted that we call Observable states ‘Observation’ & Hidden states ‘States’.\n\nBelow are the specified all the components of HMM<br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1240/format:webp/1*ARltONawvqjzKeZOMvD-tg.png\" height=100 /><br>\n> *$Q$: Set of possible Tags<br><br>\n$A$: The A matrix contains the tag transition probabilities<br><br>\n$P$($ti$|$ti−1$) which represent the probability of a tag occurring given the previous tag. Example: Calculating A[Verb][Noun]:\n$P$ (Noun|Verb): Count(Noun & Verb)/Count(Verb)<br><br>\n$O$: Sequence of observation (words in the sentence)<br><br>\n$B$: The $B$ emission probabilities, $P(wi|ti)$, represent the probability, given a tag (say Verb), that it will be associated with a given word (say Playing). The emission probability $B$[Verb][Playing] is calculated using:<br><br>\n$P$(Playing | Verb): Count (Playing & Verb)/ Count (Verb)<br><br>\nIt must be noted that we get all these Count() from the corpus itself used for training.<br><br>\nA sample HMM with both ‘A’ & ‘B’ matrices will look like this :*\n\n<img src=\"https://miro.medium.com/v2/resize:fit:1026/format:webp/1*TY_h8WgfRH7iJy1PZKN5UQ.png\" height=100 />\n\n> *Here, the black, straight arrows represent values of Transition matrix ‘A’ while the dotted black arrow represents Emission Matrix ‘B’ for a system with Q: {MD, VB, NN}.*<br>\n\nThe writer has also explained about Decoding using HMMs.<br>\nWhile skimming through the upcoming content (Viterbi Algorithm) of the same writer, I thought it was necessary to go through this part as well.<br>\n> Given an input as HMM (Transition Matrix, Emission Matrix) and a sequence of observations $O = o1, o2, …, oT$ (Words in sentences of a corpus), find the most probable sequence of states $Q = q1q2q3…, qT$ (POS Tags in our case)<br>\nThe two major assumptions followed while decoding tag sequence using HMMs:\n> - The probability of a word appearing depends only on its **own tag** and is independent of neighboring words and tags.\n> - The probability of a tag depends only on ***the previous tag(bigram HMM)*** that occured rather than the entire previous tag sequence i.e. shows Markov Property. Though we can be flexible with this.\n\nWhich, I believe, means unlikely to Markov Chain, it references on the previous tag, but surely, not the entire previous tags.<br>\nLet's move on to Viterbi Algorithm\n\n### Viterbi Algorithm\n[Mehul Gupta](https://medium.com/@mehulgupta_7991) has also well explained about Viterbi Algorithm from the same [article](https://medium.com/data-science-in-your-pocket/pos-tagging-using-hidden-markov-models-hmm-viterbi-algorithm-in-nlp-mathematics-explained-d43ca89347c4).<br><br>\nViterbi Algorithm is a decoding algorithm used for HMMs.<br>\nThe writer mentioned that setting up Lattice, the probability matrix, is necessary.<br>\nIn prior to proceed further, being familiar with the tags of Part of Speech would be necessary.<br><br>\n\n<img src=\"https://m-clark.github.io/text-analysis-with-R/img/POS-Tags.png\" height=200 /> <br>\n[***Source: Text Analysis in R by Michael Clark***](https://m-clark.github.io/text-analysis-with-R/part-of-speech-tagging.html)<br>\n\n    \nWith a sample sentence ***Janet will back the bill***, it will look like this on Lattice:<br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1080/format:webp/1*8-5KZVj-_jZOWN83gGhD5A.png\" height=100 />\n\nAs all the words in this sentence are commonly used words, there are no word with an \"Unknown\" tag.<br><br>\n\nEach cell of the lattice is represented by $V_t(j)$, $V$ for Viterbi, $t$ for column, $j$ for row.<br>\nThis represents probability that the HMM is in $state j(present POS Tag)$ after seeing the $first t observations (past words for which lattice values has been calculated)$.<br>\nThis passes through the most **probable state sequence (Previous POS Tag)** $q_1, q_2, ... q_t-1$.<br>\nWhich means, if we have the word **back**, we most probably will have **Janet** and **will** in previous order.<br>\n\n$V_t(j)$ is calculated as :\n$$V_t(j) = max: V_t-1*a(i,j)* b_j(O_t)$$\nwhere we got ‘a’(transition matrix) & ‘b’(emission matrix) from the HMM part calculations discussed above.:<br>\n<img src=\"https://miro.medium.com/v2/resize:fit:1014/format:webp/1*1UylhpDw7suhH9WpnPYFaw.png\" height=20 />","metadata":{}},{"cell_type":"markdown","source":"### Main Functions\n-----------------------------------\n\n### 1. Cut\n-----------------------------------\n***This section shows how to use jieba.cut method.***<br>\n- The `jieba.cut` function accepts three input parameters: the first parameter is the string to be cut; the second parameter is `cut_all`, controlling the cut mode; the third parameter is to control whether to use the Hidden Markov Model.\n- `jieba.cut_for_search` accepts two parameter: the string to be cut; whether to use the Hidden Markov Model. This will cut the sentence into short words suitable for search engines.\n- The input string can be an unicode/str object, or a str/bytes object which is encoded in UTF-8 or GBK. Note that using GBK encoding is not recommended because it may be unexpectly decoded as UTF-8.\n- `jieba.cut` and `jieba.cut_for_search` returns an generator, from which you can use a `for` loop to get the segmentation result (in unicode).\n- `jieba.lcut` and `jieba.lcut_for_search` returns a list.\n- `jieba.Tokenizer(dictionary=DEFAULT_DICT)` creates a new customized Tokenizer, which enables you to use different dictionaries at the same time. `jieba.dt` is the default Tokenizer, to which almost all global functions are mapped.\n<br>","metadata":{}},{"cell_type":"code","source":"print(\"Code example: Segmentation\\n\\nOutput: \")\n\n#encoding=utf-8\nimport jieba\n\nseg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\nprint(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\nprint()\nseg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\nprint(\"Default Mode: \" + \"/ \".join(seg_list))  # 默认模式\nprint()\nseg_list = jieba.cut(\"他来到了网易杭研大厦\")\nprint(\", \".join(seg_list))\nprint()\nseg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\nprint(\", \".join(seg_list))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:38.600752Z","iopub.execute_input":"2023-09-13T01:00:38.601119Z","iopub.status.idle":"2023-09-13T01:00:38.616458Z","shell.execute_reply.started":"2023-09-13T01:00:38.601089Z","shell.execute_reply":"2023-09-13T01:00:38.615137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Add a custom dictionary\n-----------------------------------\n***This section explains about how to load & modifying the dictionary***<br>\n#### Load dictionary\n- Developers can specify their own custom dictionary to be included in the jieba default dictionary. Jieba is able to identify new words, but you can add your own new words can ensure a higher accuracy.\n- Usage: `jieba.load_userdict(file_name)` # file_name is a file-like object or the path of the custom dictionary\n- The dictionary format is the same as that of `dict.txt`: one word per line; each line is divided into three parts separated by a space: word, word frequency, POS tag. If `file_name` is a path or a file opened in binary mode, the dictionary must be UTF-8 encoded.\n- The word frequency and POS tag can be omitted respectively. The word frequency will be filled with a suitable value if omitted.\n\n**Example:** <br>\n*创新办 3 i*<br>\n*云计算 5<br>\n凱特琳 nz<br>\n台中<br>*\n\n- Change a Tokenizer's `tmp_dir` and `cache_file` to specify the path of the cache file, for using on a restricted file system.\n\n**Example:** <br>\n*云计算 5<br>\n  李小福 2<br>\n  创新办 3<br>\n  [Before]： 李小福 / 是 / 创新 / 办 / 主任 / 也 / 是 / 云 / 计算 / 方面 / 的 / 专家 /<br>\n  [After]：　李小福 / 是 / 创新办 / 主任 / 也 / 是 / 云计算 / 方面 / 的 / 专家 /*<br>\n  \n#### Modify dictionary\n- Use add_word(word, freq=None, tag=None) and del_word(word) to modify the dictionary dynamically in programs.\n- Use suggest_freq(segment, tune=True) to adjust the frequency of a single word so that it can (or cannot) be segmented.\n- Note that HMM may affect the final result.\n","metadata":{}},{"cell_type":"code","source":"print(\"Example :\\n\")\n>>> print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))\n# 如果/放到/post/中将/出错/。\n>>> jieba.suggest_freq(('中', '将'), True)\n# 494\n>>> print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))\n# 如果/放到/post/中/将/出错/。\n>>> print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))\n# 「/台/中/」/正确/应该/不会/被/切开\n>>> jieba.suggest_freq('台中', True)\n# 69\n>>> print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))\n# 「/台中/」/正确/应该/不会/被/切开\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:38.618894Z","iopub.execute_input":"2023-09-13T01:00:38.619609Z","iopub.status.idle":"2023-09-13T01:00:38.632659Z","shell.execute_reply.started":"2023-09-13T01:00:38.619560Z","shell.execute_reply":"2023-09-13T01:00:38.631156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Keyword Extraction\n-----------------------------------\n***This section explains how to extract keywords***<br>\n\n`import jieba.analyse`\n\n- `jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())`<br>\n    - `sentence`: the text to be extracted<br>\n    - `topK`: return how many keywords with the highest TF/IDF weights. The default value is 20<br>\n`withWeight`: whether return TF/IDF weights with the keywords. The default value is False<br>\n    - `allowPOS`: filter words with which POSs are included. Empty for no filtering.<br>\n- `jieba.analyse.TFIDF(idf_path=None)` creates a new TF/IDF instance, `idf_path` specifies IDF file path.<br><br>\n\n**Example (keyword extraction)**<br>\nhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags.py<br>\nDevelopers can specify their own custom IDF corpus in jieba keyword extraction<br>\n- Usage: `jieba.analyse.set_idf_path(file_name)`<br>\n`file_name` is the path for the custom corpus<br>\n- Custom Corpus Sample: (not working)<br>\nhttps://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big\n- Sample Code:<br>\nhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py<br>\nDevelopers can specify their own custom stop words corpus in jieba keyword extraction\n\n- Usage: `jieba.analyse.set_stop_words(file_name)`<br>\n`file_name` is the path for the custom corpus\n- Custom Corpus Sample:<br>\nhttps://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt\n- Sample Code:<br>\nhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py<br>\n\nThere's also a [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) implementation available.\n\n- Use: `jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))`\n\nNote that it filters POS by default.<br>\n`jieba.analyse.TextRank()` creates a new TextRank instance.\n","metadata":{}},{"cell_type":"markdown","source":"### 4. Part of Speech Tagging\n-----------------------------------\n***This section explains how to tag Part of Words***<br>\n\n- `jieba.posseg.POSTokenizer(tokenizer=None)` creates a new customized Tokenizer.<br>\n`tokenizer` specifies the `jieba.Tokenizer` to internally use. jieba.posseg.dt is the default POSTokenizer.\n- Tags the POS of each word after segmentation, using labels compatible with ictclas.<br>\n\n**Example:**","metadata":{}},{"cell_type":"code","source":">>> import jieba.posseg as pseg\n>>> words = pseg.cut(\"我爱北京国安\")\n>>> for w in words:\n...    print('%s %s' % (w.word, w.flag))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:38.634981Z","iopub.execute_input":"2023-09-13T01:00:38.635484Z","iopub.status.idle":"2023-09-13T01:00:39.354590Z","shell.execute_reply.started":"2023-09-13T01:00:38.635441Z","shell.execute_reply":"2023-09-13T01:00:39.353328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Parallel Processing\n-----------------------------------\n***This section explains about parallel processing.***<br><br>\nI believe this would be helpful on large dataset, but I will not implement this module on this notebook.<br>\n\n- Principle: Split target text by line, assign the lines into multiple Python processes, and then merge the results, which is considerably faster.\n\n- Based on the multiprocessing module of Python.\n\n- Usage:\n\n    - `jieba.enable_parallel(4)`<br>\n    Enable parallel processing. The parameter is the number of processes.\n    - `jieba.disable_parallel()`<br>\n    Disable parallel processing.\n    \n- **Example:** https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py\n\n- Result: On a four-core 3.4GHz Linux machine, do accurate word segmentation on Complete Works of Jin Yong, and the speed reaches 1MB/s, which is 3.3 times faster than the single-process version.\n\n- Note that parallel processing supports only default tokenizers, `jieba.dt` and `jieba.posseg.dt`.\n","metadata":{}},{"cell_type":"markdown","source":"### 6. Tokenize: return words with position\n-----------------------------------\n***This section explains about tokenizing words.***\n- The input must be unicode\n","metadata":{}},{"cell_type":"code","source":"print('Default Mode\\n')\nresult = jieba.tokenize(u'永和服装饰品有限公司')\nfor tk in result:\n    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:39.356287Z","iopub.execute_input":"2023-09-13T01:00:39.356609Z","iopub.status.idle":"2023-09-13T01:00:39.363986Z","shell.execute_reply.started":"2023-09-13T01:00:39.356581Z","shell.execute_reply":"2023-09-13T01:00:39.362507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Search Mode\\n\")\nresult = jieba.tokenize(u'永和服装饰品有限公司',mode='search')\nfor tk in result:\n    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:39.366017Z","iopub.execute_input":"2023-09-13T01:00:39.366558Z","iopub.status.idle":"2023-09-13T01:00:39.379671Z","shell.execute_reply.started":"2023-09-13T01:00:39.366511Z","shell.execute_reply":"2023-09-13T01:00:39.378266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. ChineseAnalyzer for Whoosh\n-----------------------------------\n***This section explains about tokenizing words.***\n\n`from jieba.analyse import ChineseAnalyzer`<br>\n**Example:** (Copy & Pasted below to see the result)<br> https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py<br>","metadata":{}},{"cell_type":"code","source":"# -*- coding: UTF-8 -*-\nfrom __future__ import unicode_literals\nimport sys,os\nsys.path.append(\"../\")\nfrom whoosh.index import create_in,open_dir\nfrom whoosh.fields import *\nfrom whoosh.qparser import QueryParser\n\nfrom jieba.analyse.analyzer import ChineseAnalyzer\n\nanalyzer = ChineseAnalyzer()\n\nschema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\nif not os.path.exists(\"tmp\"):\n    os.mkdir(\"tmp\")\n\nix = create_in(\"tmp\", schema) # for create new index\n#ix = open_dir(\"tmp\") # for read only\nwriter = ix.writer()\n\nwriter.add_document(\n    title=\"document1\",\n    path=\"/a\",\n    content=\"This is the first document we’ve added!\"\n)\n\nwriter.add_document(\n    title=\"document2\",\n    path=\"/b\",\n    content=\"The second one 你 中文测试中文 is even more interesting! 吃水果\"\n)\n\nwriter.add_document(\n    title=\"document3\",\n    path=\"/c\",\n    content=\"买水果然后来世博园。\"\n)\n\nwriter.add_document(\n    title=\"document4\",\n    path=\"/c\",\n    content=\"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作\"\n)\n\nwriter.add_document(\n    title=\"document4\",\n    path=\"/c\",\n    content=\"咱俩交换一下吧。\"\n)\n\nwriter.commit()\nsearcher = ix.searcher()\nparser = QueryParser(\"content\", schema=ix.schema)\n\nfor keyword in (\"水果世博园\",\"你\",\"first\",\"中文\",\"交换机\",\"交换\"):\n    print(\"result of \",keyword)\n    q = parser.parse(keyword)\n    results = searcher.search(q)\n    for hit in results:\n        print(hit.highlights(\"content\"))\n    print(\"=\"*10)\n\nfor t in analyzer(\"我的好朋友是李明;我爱北京国安;IBM和Microsoft; I have a dream. this is intetesting and interested me a lot\"):\n    print(t.text)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:39.381456Z","iopub.execute_input":"2023-09-13T01:00:39.381866Z","iopub.status.idle":"2023-09-13T01:00:40.017945Z","shell.execute_reply.started":"2023-09-13T01:00:39.381835Z","shell.execute_reply":"2023-09-13T01:00:40.016035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Command Line Interface\n-----------------------------------\n```\n$> python -m jieba --help\nJieba command line interface.\n\npositional arguments:\n  filename              input file\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d [DELIM], --delimiter [DELIM]\n                        use DELIM instead of ' / ' for word delimiter; or a\n                        space if it is used without DELIM\n  -p [DELIM], --pos [DELIM]\n                        enable POS tagging; if DELIM is specified, use DELIM\n                        instead of '_' for POS delimiter\n  -D DICT, --dict DICT  use DICT as dictionary\n  -u USER_DICT, --user-dict USER_DICT\n                        use USER_DICT together with the default dictionary or\n                        DICT (if specified)\n  -a, --cut-all         full pattern cutting (ignored with POS tagging)\n  -n, --no-hmm          don't use the Hidden Markov Model\n  -q, --quiet           don't print loading messages to stderr\n  -V, --version         show program's version number and exit\n\nIf no filename specified, use STDIN instead.\n\n```","metadata":{}},{"cell_type":"markdown","source":"## Initialization\n-----------------------------------\n\nBy default, Jieba don't build the prefix dictionary unless it's necessary. This takes 1-3 seconds, after which it is not initialized again.<br>\nIf you want to initialize Jieba manually, you can call:\n","metadata":{}},{"cell_type":"code","source":"import jieba\njieba.initialize()  # (optional)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.019146Z","iopub.execute_input":"2023-09-13T01:00:40.019546Z","iopub.status.idle":"2023-09-13T01:00:40.027014Z","shell.execute_reply.started":"2023-09-13T01:00:40.019515Z","shell.execute_reply":"2023-09-13T01:00:40.025540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also specify the dictionary (not supported before version 0.28) :","metadata":{}},{"cell_type":"code","source":"jieba.set_dictionary('/kaggle/input/ideogram-phonogram-dataset/dict.txt.big')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.028512Z","iopub.execute_input":"2023-09-13T01:00:40.029105Z","iopub.status.idle":"2023-09-13T01:00:40.044429Z","shell.execute_reply.started":"2023-09-13T01:00:40.029073Z","shell.execute_reply":"2023-09-13T01:00:40.042706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Other Dictionaries\n-----------------------------------\nIt is possible to use your own dictionary with Jieba, and there are also two dictionaries ready for download:<br>\n1. A smaller dictionary for a smaller memory footprint: <br>\nhttps://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small<br>\n\n2. There is also a bigger dictionary that has better support for traditional Chinese (繁體):<br>\nhttps://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big<br>\n***You can find both files from [here](https://www.kaggle.com/datasets/jasonheesanglee/ideogram-phonogram-dataset)***<br><br>\n\nBy default, an in-between dictionary is used, called `dict.txt` and included in the distribution.<br>\n\nIn either case, download the file you want, and then call `jieba.set_dictionary('data/dict.txt.big')` or just replace the existing `dict.txt`.\n\n","metadata":{}},{"cell_type":"markdown","source":"## jieba/jieba\n-----------------------------------\nAs I have finished going through README.md, I will start on the original plan.<br>\nBelow is how jieba/jieba directory looks like.<br><br>\nCode explanation done with the help of ChatGPT & BARD\n<img src=\"https://github.com/jasonheesanglee/Ideogram_Phonogram/blob/main/IDEOPHONO/jieba_jieba.png?raw=true\" height=\"100\" />","metadata":{}},{"cell_type":"markdown","source":"### dict.txt\n-----------------------------------\nLet's take a look at dict.txt<br>\nFrom the code output below, we can see that this txt file is composed in a format of `[word]` | `[word frequency]` | `[POS]` as explained in [2.Add a custom dictionary](https://www.kaggle.com/code/jasonheesanglee/ideogram-based-vs-phonogram-based-language?scriptVersionId=142722802&cellId=36).","metadata":{}},{"cell_type":"code","source":"with open(r'/kaggle/input/ideogram-phonogram-dataset/dict.txt') as dict_txt:\n    display(dict_txt.readlines()[0:10])\n#     display(dict_txt.read()[:])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-13T01:00:40.046580Z","iopub.execute_input":"2023-09-13T01:00:40.047216Z","iopub.status.idle":"2023-09-13T01:00:40.196820Z","shell.execute_reply.started":"2023-09-13T01:00:40.047135Z","shell.execute_reply":"2023-09-13T01:00:40.195788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### _compat.py\n-----------------------------------\nI will start with _compat.py as both `main.py` and `init.py` starts by importing this module.<br>\n(Sorry for not including \"___\" in the file name... Hate markdown syntax..)<br>\n\nI will break the module down per `def`.","metadata":{}},{"cell_type":"markdown","source":"### Importing Modules\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport logging\nimport os\nimport sys","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.198427Z","iopub.execute_input":"2023-09-13T01:00:40.199454Z","iopub.status.idle":"2023-09-13T01:00:40.206095Z","shell.execute_reply.started":"2023-09-13T01:00:40.199419Z","shell.execute_reply":"2023-09-13T01:00:40.204504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logging Configurations\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"log_console = logging.StreamHandler(sys.stderr)\ndefault_logger = logging.getLogger(__name__)\ndefault_logger.setLevel(logging.DEBUG)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.198427Z","iopub.execute_input":"2023-09-13T01:00:40.199454Z","iopub.status.idle":"2023-09-13T01:00:40.206095Z","shell.execute_reply.started":"2023-09-13T01:00:40.199419Z","shell.execute_reply":"2023-09-13T01:00:40.204504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `log_console` is created as a Stream Handler that directs log messages to the standard error ('sys.stderr')<br>\n- `default_loger` is a logger object created for the current module.<br>\n- \\_\\_name__ refers to the current module (`_compat.py`)<br>\nIt is configured to log messages with a minimum level of \"DEBUG\"","metadata":{}},{"cell_type":"markdown","source":"### `setLogLevel`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def setLogLevel(log_level):\n#     default_logger.setLevel(log_level)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.213045Z","iopub.execute_input":"2023-09-13T01:00:40.214473Z","iopub.status.idle":"2023-09-13T01:00:40.225907Z","shell.execute_reply.started":"2023-09-13T01:00:40.214417Z","shell.execute_reply":"2023-09-13T01:00:40.224519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function is defined to allow changing the log level of the \"default_logger\".<br>\nBy calling this function with a log level. the logger's level will be set.<br>\nFor example, if the log level is set to `logging.INFO`, the log level will be changed to `INFO`, and the logger will only display messages of INFO level.","metadata":{}},{"cell_type":"code","source":"# check_paddle_install = {'is_paddle_installed': False}\n\n# try:\n#     import pkg_resources\n\n#     get_module_res = lambda *res: pkg_resources.resource_stream(__name__,\n#                                                                 os.path.join(*res))\n# except ImportError:\n#     get_module_res = lambda *res: open(os.path.normpath(os.path.join(\n#         os.getcwd(), os.path.dirname(__file__), *res)), 'rb')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.227542Z","iopub.execute_input":"2023-09-13T01:00:40.227942Z","iopub.status.idle":"2023-09-13T01:00:40.241732Z","shell.execute_reply.started":"2023-09-13T01:00:40.227907Z","shell.execute_reply":"2023-09-13T01:00:40.240257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This part is to check whether the PaddlePaddle library is installed.<br>\n- If `pkg_resources` can be imported, it sets `is_paddle_installed` to True.<br>\n- It uses pkg_resources.resource_stream if available, and if not, it constructs the resource path using os.getcwd() and os.path.dirname(__file__) and opens the resource as a binary file.","metadata":{}},{"cell_type":"markdown","source":"### `enable_paddle`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def enable_paddle():\n#     try:\n#         import paddle\n#     except ImportError:\n#         default_logger.debug(\"Installing paddle-tiny, please wait a minute......\")\n#         os.system(\"pip install paddlepaddle-tiny\")\n#         try:\n#             import paddle\n#         except ImportError:\n#             default_logger.debug(\n#                 \"Import paddle error, please use command to install: pip install paddlepaddle-tiny==1.6.1.\"\n#                 \"Now, back to jieba basic cut......\")\n#     if paddle.__version__ < '1.6.1':\n#         default_logger.debug(\"Find your own paddle version doesn't satisfy the minimum requirement (1.6.1), \"\n#                              \"please install paddle tiny by 'pip install --upgrade paddlepaddle-tiny', \"\n#                              \"or upgrade paddle full version by \"\n#                              \"'pip install --upgrade paddlepaddle (-gpu for GPU version)' \")\n#     else:\n#         try:\n#             import jieba.lac_small.predict as predict\n#             default_logger.debug(\"Paddle enabled successfully......\")\n#             check_paddle_install['is_paddle_installed'] = True\n#         except ImportError:\n#             default_logger.debug(\"Import error, cannot find paddle.fluid and jieba.lac_small.predict module. \"\n#                                  \"Now, back to jieba basic cut......\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.243675Z","iopub.execute_input":"2023-09-13T01:00:40.244887Z","iopub.status.idle":"2023-09-13T01:00:40.256933Z","shell.execute_reply.started":"2023-09-13T01:00:40.244846Z","shell.execute_reply":"2023-09-13T01:00:40.255482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This function begins by importing `paddle`.<br>\n- If the nested import raises an ImportError again, it logs another message to the default logger, indicating that PaddlePaddle couldn't be imported even after the installation and suggests a specific command to install a particular version of PaddlePaddle.\n- The function does not return any values but effectively determines whether PaddlePaddle is available for use in the Jieba library and logs relevant messages.\n- This function handles the installation and availability of the PaddlePaddle library, which may be used by Jieba for certain tasks.<br> If PaddlePaddle is available and of the correct version, it sets the is_paddle_installed flag to True, indicating that PaddlePaddle support is enabled.<br> Otherwise, it falls back to the basic Jieba functionality.","metadata":{}},{"cell_type":"markdown","source":"### `PaddlePaddle`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# PY2 = sys.version_info[0] == 2\n\n# default_encoding = sys.getfilesystemencoding()\n\n# if PY2:\n#     text_type = unicode\n#     string_types = (str, unicode)\n\n#     iterkeys = lambda d: d.iterkeys()\n#     itervalues = lambda d: d.itervalues()\n#     iteritems = lambda d: d.iteritems()\n\n# else:\n#     text_type = str\n#     string_types = (str,)\n#     xrange = range\n\n#     iterkeys = lambda d: iter(d.keys())\n#     itervalues = lambda d: iter(d.values())\n#     iteritems = lambda d: iter(d.items())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.259092Z","iopub.execute_input":"2023-09-13T01:00:40.260212Z","iopub.status.idle":"2023-09-13T01:00:40.278854Z","shell.execute_reply.started":"2023-09-13T01:00:40.260142Z","shell.execute_reply":"2023-09-13T01:00:40.277485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section deals with defining variables and functions based on Python version compatibility (Python 2 and Python 3).\n\n- `PY2 = sys.version_info[0] == 2`:<br>This line determines whether the Python version being used is Python 2.<br>It checks if the major version number (`sys.version_info[0]`) is equal to 2 and assigns the result to the variable PY2.\n- `default_encoding = sys.getfilesystemencoding()`:This line obtains the default encoding used by the file system and assigns it to the variable `default_encoding`.<br>This is often used for encoding and decoding file paths.\n\n***I will pass the first `if` statement as we are using Python 3***\n\n- `text_type = str`: In Python 3, str is used for representing both byte strings and Unicode strings, so it assigns the name text_type to str.\n- `string_types = (str,)`: It defines string_types as a tuple containing only str since there's no need for unicode in Python 3.\n- `xrange = range`: In Python 2, there was a separate xrange function for creating efficient iterators over a range of numbers.<br>In Python 3, the range function provides the same functionality, so it assigns range to xrange.","metadata":{}},{"cell_type":"markdown","source":"### `strdecode`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def strdecode(sentence):\n#     if not isinstance(sentence, text_type):\n#         try:\n#             sentence = sentence.decode('utf-8')\n#         except UnicodeDecodeError:\n#             sentence = sentence.decode('gbk', 'ignore')\n#     return sentence","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.280576Z","iopub.execute_input":"2023-09-13T01:00:40.281662Z","iopub.status.idle":"2023-09-13T01:00:40.294530Z","shell.execute_reply.started":"2023-09-13T01:00:40.281628Z","shell.execute_reply":"2023-09-13T01:00:40.293444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `strdecode` function decodes string to ensure they are in utf-8 format.\n- If it is not utf-8 format, it decodes the sentence with `gbk` encoding.","metadata":{}},{"cell_type":"markdown","source":"### `resolve_filename`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"# def resolve_filename(f):\n#     try:\n#         return f.name\n#     except AttributeError:\n#         return repr(f)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T01:00:40.296412Z","iopub.execute_input":"2023-09-13T01:00:40.296765Z","iopub.status.idle":"2023-09-13T01:00:40.317412Z","shell.execute_reply.started":"2023-09-13T01:00:40.296725Z","shell.execute_reply":"2023-09-13T01:00:40.315765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `resolve_filename` defines a function named resolve_filename that takes one argument called f, which is expected to be a file object.\n- If the name attribute is not available, it returns a string representation of the file object f using the repr() function.<br>This representation includes information about the object, which can be helpful for debugging or providing more context.","metadata":{}},{"cell_type":"markdown","source":"### __main__.py\n-----------------------------------\nI have hid this part of analysis as it is mostly configurations.","metadata":{}},{"cell_type":"markdown","source":"### Importing modules\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"\"\"\"Jieba command line interface.\"\"\"\n\n# import sys\n# import jieba\n# from argparse import ArgumentParser\n# from ._compat import *","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Argument Parsing\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# parser = ArgumentParser(usage=\"%s -m jieba [options] filename\" % sys.executable, description=\"Jieba command line interface.\", epilog=\"If no filename specified, use STDIN instead.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section sets up the argument parser for the command-line interface of `jieba`.<br>\nIt defines various command-line options that can be used when running the script.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Argument Definitions\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# parser.add_argument(\"-d\", \"--delimiter\", metavar=\"DELIM\", default=' / ',\n#                     nargs='?', const=' ',\n#                     help=\"use DELIM instead of ' / ' for word delimiter; or a space if it is used without DELIM\")\n# parser.add_argument(\"-p\", \"--pos\", metavar=\"DELIM\", nargs='?', const='_',\n#                     help=\"enable POS tagging; if DELIM is specified, use DELIM instead of '_' for POS delimiter\")\n# parser.add_argument(\"-D\", \"--dict\", help=\"use DICT as dictionary\")\n# parser.add_argument(\"-u\", \"--user-dict\",\n#                     help=\"use USER_DICT together with the default dictionary or DICT (if specified)\")\n# parser.add_argument(\"-a\", \"--cut-all\",\n#                     action=\"store_true\", dest=\"cutall\", default=False,\n#                     help=\"full pattern cutting (ignored with POS tagging)\")\n# parser.add_argument(\"-n\", \"--no-hmm\", dest=\"hmm\", action=\"store_false\",\n#                     default=True, help=\"don't use the Hidden Markov Model\")\n# parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", default=False,\n#                     help=\"don't print loading messages to stderr\")\n# parser.add_argument(\"-V\", '--version', action='version',\n#                     version=\"Jieba \" + jieba.__version__)\n# parser.add_argument(\"filename\", nargs='?', help=\"input file\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section add arguments to the parser.\n- `-d` is used to specify a delimiter for word.\n- `-p` is used to enable part of speech tagging.\n- `-D` allows specifying a custom dictionary.\n- `-u` is for a user-defined dictionary.\n- `-a` enables full pattern cutting.\n- `-n` disables the Hidden Markov Model.\n- `-q` makes the script run quietly without loading messages.\n\n***I guess these are the similar terms and functions to*** `!pip install -q ...`.<br>","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Parsing Command-Line Arguments\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# args = parser.parse_args()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section parses the command-line arguments using the previously defined argument parser.<br>\n- The parsed arguments are stored in the `args` variable, which is an object with attributes corresponding to the defined arguments.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Configuration based on Command-Line Arguments.\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# if args.quiet:\n#     jieba.setLogLevel(60)\n\n# if args.pos:\n#     import jieba.posseg\n#     posdelim = args.pos\n#     def cutfunc(sentence, _, HMM=True):\n#         for w, f in jieba.posseg.cut(sentence, HMM):\n#             yield w + posdelim + f\n# else:\n#     cutfunc = jieba.cut","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If the `-q` flag is provided in the command line, it sets the logging level of the jieba library to 60 (which corresponds to the `CRITICAL` log level.<br>\n- This means that loading messages will not be printed to the standard error (stderr)<br>\n\n- If the `-p` flag is provided in the command line, it imports the `jieba.posseg` module and sets up a custom word segmentation function (`cutfunc`) that incorporates POS tags based on the specified delimiter.\n- If the `-p` flag is not provided, it sets `cutfunc` to the default word segmentation function (`jieba.cut`)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Variable Assignments\n-----------------------------------","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# delim = text_type(args.delimiter)\n# cutall = args.cutall\n# hmm = args.hmm\n# fp = open(args.filename, 'r') if args.filename else sys.stdin","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `delim` converts the specified delimiter into appropriate text type. (Either Unicode or byte string)\n- `cutall` stores whether the `-a` flag was provided.\n- `hmm` stores whether `-n` flag was provided.\n- `fp` opens the input file specified in the command line `arg.filename` or `sys.stdin` if filename is not provided.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### `jieba` Configuration\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# if args.dict:\n#     jieba.initialize(args.dict)\n# else:\n#     jieba.initialize()\n# if args.user_dict:\n#     jieba.load_userdict(args.user_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If the `-D` flag is provided, it initializes jieba with the specified dictionary.<br>Otherwise, it uses the default dictionary.\n- If the `-u` flag is provided, it loads the specified user dictionary.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Processing and Output\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# ln = fp.readline()\n# while ln:\n#     l = ln.rstrip('\\r\\n')\n#     result = delim.join(cutfunc(ln.rstrip('\\r\\n'), cutall, hmm))\n#     if PY2:\n#         result = result.encode(default_encoding)\n#     print(result)\n#     ln = fp.readline()\n\n# fp.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.322142Z","iopub.status.idle":"2023-09-12T12:30:44.322653Z","shell.execute_reply.started":"2023-09-12T12:30:44.322387Z","shell.execute_reply":"2023-09-12T12:30:44.322412Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section reads lines from the input file (or stdin if no filename is provided) using fp.readline().\n- It applies the word segmentation function (cutfunc) to each line, joining the resulting tokens with the specified delimiter.\n- If the Python version is 2.x (PY2 is True), it encodes the result using the default encoding.\n- It prints the segmented and possibly encoded text to the standard output.\n- This process continues until there are no more lines to read.\n- Finally, it closes the input file (if opened).","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### __init__.py\n-----------------------------------","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# from __future__ import absolute_import, unicode_literals\n1\n# import marshal\n# import re\n# import tempfile\n# import threading\n# import time\n# from hashlib import md5\n# from math import log\n\n# from . import finalseg\n# from ._compat import *","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section imports various modules and packages needed for the functionality of `jieba`, including `marshal`, `re`, `tempfile`, `threading`, `time`, `md5`, `log`, and modules from within the `jieba` package itself.\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Version & Licence\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# __version__ = '0.42.1'\n# __license__ = 'MIT'","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section specifies the Version & License of `jieba`","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Conditional Import\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# if os.name == 'nt':\n#     from shutil import move as _replace_file\n# else:\n#     _replace_file = os.rename","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section imports `move` from `shutil` and place in variable `_replace_file` if this code runs on Windows NT.\n- This section place `os.rename` in variable `_replace_file`.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Path Normalization Function\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# _get_abs_path = lambda path: os.path.normpath(os.path.join(os.getcwd(), path))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section normalizes the path of the destination.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Constants\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# DEFAULT_DICT = None\n# DEFAULT_DICT_NAME = \"dict.txt\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`DEFAULT_DICT` set as `None`<br>\n`DEFAULT_DICT_NAME` set as dict.txt which is in `jieba/jieba/`.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Logger Setup\n-----------------------------------","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"# log_console = logging.StreamHandler(sys.stderr)\n# default_logger = logging.getLogger(__name__)\n# default_logger.setLevel(logging.DEBUG)\n# default_logger.addHandler(log_console)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- These lines set up a logger (`default_logger`) to log messages with a log level of `DEBUG` to the standard error stream (`sys.stderr`).<br>This logger is used for debugging and logging informational messages.\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Dict, Parallel Processing, Regular Expression\n-----------------------------------","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"# DICT_WRITING = {}\n\n# pool = None","metadata":{"_kg_hide-output":true,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `DICT_WRITING` is used to keep track of whether a dictionary file is currently being written. <br>It's an empty dictionary initially.<br><br>\n- `pool` variable is used later in the code for parallel processing with multiprocessing.<br>Here, it's set to None to indicate that no multiprocessing pool has been created yet.","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"markdown","source":"### Regular Expression\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# re_userdict = re.compile('^(.+?)( [0-9]+)?( [a-z]+)?$', re.U)\n# re_eng = re.compile('[a-zA-Z0-9]', re.U)\n\n# # \\u4E00-\\u9FD5a-zA-Z0-9+#&\\._ : All non-space characters. Will be handled with re_han\n# # \\r\\n|\\s : whitespace characters. Will not be handled.\n# # re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%]+)\", re.U)\n# # Adding \"-\" symbol in re_han_default\n\n# re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n# re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `re_userdict`: This line defines `re_userdict`, a regular expression.<br>\n    - `^`: Matches the start of a line.\n    - `(.+?)`: Matches one or more characters (non-greedy) and captures them. <br>This is used to capture the word.\n    - `( [0-9]+)?`: Matches an optional space followed by one or more digits (captures them). <br>This is used to capture the frequency of the word (if present).\n    - `( [a-z]+)?`: Matches an optional space followed by one or more lowercase letters (captures them). <br>This is used to capture the part of speech tag (if present).\n    - `re.U`: This is a flag that specifies Unicode matching, enabling the regular expression to work with Unicode characters.<br><br>\n- `re_eng`: It's used to match English alphanumeric characters.<br>\n    - `[a-zA-Z0-9]`: Matches any single English alphabet (lowercase or uppercase) or digit.\n    - `re.U`: This is the Unicode matching flag, enabling the regular expression to work with Unicode characters.<br><br>\n    \n- `re_han_default` : It uses re.compile to create a regular expression pattern object.<br>\n    - `([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%]+)`: This part of the regular expression defines a character group that matches one or more non-space characters.<br>It includes Chinese characters `(\\u4E00-\\u9FD5)` (from , alphanumeric characters (a-zA-Z0-9), and specific symbols (+#&\\._%).\n    - re.U: This is a flag that specifies Unicode matching, enabling the regular expression to work with Unicode characters.<br><br>\n- `re_skip_default` : It is used for matching whitespace characters and line breaks `(\\r\\n)`.<br>These characters are not considered part of words during word segmentation.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### `setLogLevel`\n-----------------------------------","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"# def setLogLevel(log_level):\n#     default_logger.setLevel(log_level)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section is to allow changing the log level of the default_logger.\n- Calling this function with a log level, and it will set the logger's level accordingly.\n- For example, if the function is called as setLogLevel(logging.INFO), it will change the log level to INFO, and the logger will only display messages of INFO level and above.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"### Tokenizer\n-----------------------------------\nI have splitted the Tokenizer class into per definition.","metadata":{}},{"cell_type":"code","source":"# class Tokenizer(object):","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def __init__`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def __init__(self, dictionary=DEFAULT_DICT):\n#         self.lock = threading.RLock()\n#         if dictionary == DEFAULT_DICT:\n#             self.dictionary = dictionary\n#         else:\n#             self.dictionary = _get_abs_path(dictionary)\n#         self.FREQ = {}\n#         self.total = 0\n#         self.user_word_tag_tab = {}\n#         self.initialized = False\n#         self.tmp_dir = None\n#         self.cache_file = None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- When calling this `Tokenizer` class, it takes dictionary as an optional parameter.\n- `self.lock` initializes a threading reentrant lock.\n    - It is used for thread synchronization to ensure that certain operations are thread-safe.\n- `DEFAULT_DICT` is set as a default dictionary, and if another dictionary is given as an input, `self.dicitonary` will return the path to the new dictionary.\n- `self.initialized` is set to False.\n    - This is used to keep track of whether the tokenizer has been initialized.\n- `self.tmp_dir` is set to `None`.\n    - It will later take a path to temporary directory.\n- `self.cache_file` is set to `None`.\n    - This as well will later take a path to cache file.","metadata":{}},{"cell_type":"markdown","source":"### `def __repr__`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def __repr__(self):\n#         return '<Tokenizer dictionary=%r>' % self.dictionary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This definition returns a string that includes information about the dictionary being used.","metadata":{}},{"cell_type":"markdown","source":"### `def gen_pfdict`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     @staticmethod\n#     def gen_pfdict(f):\n#         lfreq = {}\n#         ltotal = 0\n#         f_name = resolve_filename(f)\n#         for lineno, line in enumerate(f, 1):\n#             try:\n#                 line = line.strip().decode('utf-8')\n#                 word, freq = line.split(' ')[:2]\n#                 freq = int(freq)\n#                 lfreq[word] = freq\n#                 ltotal += freq\n#                 for ch in xrange(len(word)):\n#                     wfrag = word[:ch + 1]\n#                     if wfrag not in lfreq:\n#                         lfreq[wfrag] = 0\n#             except ValueError:\n#                 raise ValueError(\n#                     'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))\n#         f.close()\n#         return lfreq, ltotal","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `@staticmethod` defines a static method within a class.\n    - Static method is a method that belongs to the class itself rather than to an instance of the class.\n    - Static method can be called on the class itself without needing to create an instance of the class.\n    - Static methods do not have access to the instance state and do not modify it.\n    - They are often used for utility functions that are related to the class but don't require access to instance-specific data.\n- In this case, `gen_pfdict` is defined as a static method.<br><br>\n- `gen_pfdict` is responsible for generating a dictionary of word frequencies from a given dictionary file.<br><br>\n\n- `f_name = resolve_filename(f)`: `f_name` is set to the resolved filename of the input file using `resolve_filename` function.\n    - This is done to ensure that the file path is correctly resolved, especially when handling custom dictionaries.<br><br>\n\n- Outer `for` loop iterates over the file, starting from line 1.\n    - `line` is stripped and decoded into UTF-8.\n    - `lfreq` dict stores word and its frequency.\n    - `ltotal` stores frequency value.<br><br>\n    \n- Inner `for` loop iterates over each character.\n    - It creates sub-word fragments for each character and check if they are in the `lfreq` dictionary.\n    - If not, it adds them with a frequency of 0.","metadata":{}},{"cell_type":"markdown","source":"### `def initialize`\n-----------------------------------\nI will divide this definition into parts.","metadata":{}},{"cell_type":"code","source":"#     def initialize(self, dictionary=None):\n#         if dictionary:\n#             abs_path = _get_abs_path(dictionary)\n#             if self.dictionary == abs_path and self.initialized:\n#                 return\n#             else:\n#                 self.dictionary = abs_path\n#                 self.initialized = False\n#         else:\n#             abs_path = self.dictionary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, it specifies the path to the dictionary.<br>\nIf using a new dictionary, it gets the path by using `_get_abs_path` function which was defined in [`__init__.py`](https://www.kaggle.com/code/jasonheesanglee/ideogram-based-vs-phonogram-based-language?scriptVersionId=143053397&cellId=114).","metadata":{}},{"cell_type":"code","source":"#         with self.lock:\n#             try:\n#                 with DICT_WRITING[abs_path]:\n#                     pass\n#             except KeyError:\n#                 pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This section ensures that multiple threads do not attempt to initialize the same dictionary file concurrently.<br>\nIt attempts to acquire a lock from the `DICT_WRITING` dictionary using the absolute path of the dictionary file.\n","metadata":{}},{"cell_type":"code","source":"#             if self.initialized:\n#                 return","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the dictionary is already initialized, it returns without reinitializing","metadata":{}},{"cell_type":"code","source":"#             default_logger.debug(\"Building prefix dict from %s ...\" % (abs_path or 'the default dictionary'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It logs a message to indicate that it is building the prefix dictionary from the specified dictionary file (`abs_path`).<br>\nIf `abs_path` is not provided, it indicates that it's using the default dictionary.","metadata":{}},{"cell_type":"code","source":"#             t1 = time.time()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recording starting time.","metadata":{}},{"cell_type":"code","source":"#             if self.cache_file:\n#                 cache_file = self.cache_file\n#             # default dictionary\n#             elif abs_path == DEFAULT_DICT:\n#                 cache_file = \"jieba.cache\"\n#             # custom dictionary\n#             else:\n#                 cache_file = \"jieba.u%s.cache\" % md5(\n#                     abs_path.encode('utf-8', 'replace')).hexdigest()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`If` `cache_file` path exists, it uses the `cache_file` path.<br>\n`Else if` abs_path is `DEFAULT_DICT`, it creates `jieba.cache` as a name of the cache file.<br>\n`Else`, when using a custom dictionary, it calculates a cache file name based on the MD5 hash of the dictionary file's absolute path to ensure uniqueness.","metadata":{}},{"cell_type":"code","source":"#             cache_file = os.path.join(\n#                 self.tmp_dir or tempfile.gettempdir(), cache_file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It constructs the full path of the cache file by combining the cache file name and the temporary directory path.","metadata":{}},{"cell_type":"code","source":"#             tmpdir = os.path.dirname(cache_file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prevent absolute path in self.cache_file","metadata":{}},{"cell_type":"code","source":"#             load_from_cache_fail = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initializes a boolean variable load_from_cache_fail to True to track whether loading from the cache file fails.","metadata":{}},{"cell_type":"code","source":"#             if os.path.isfile(cache_file) and (abs_path == DEFAULT_DICT or\n#                                                os.path.getmtime(cache_file) > os.path.getmtime(abs_path)):\n#                 default_logger.debug(\n#                     \"Loading model from cache %s\" % cache_file)\n#                 try:\n#                     with open(cache_file, 'rb') as cf:\n#                         self.FREQ, self.total = marshal.load(cf)\n#                     load_from_cache_fail = False\n#                 except Exception:\n#                     load_from_cache_fail = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It checks if the cache file already exists and whether it's up-to-date.<br>If the dictionary is the default dictionary (`abs_path == DEFAULT_DICT`) or if the cache file's modification time is later than the dictionary file's modification time, it indicates that the cache file is valid.<br><br>\n\nIf the cache file is valid, it logs a message indicating that it's loading the model from the cache file.<br>Then, it attempts to load the precomputed word frequencies (`self.FREQ`) and the total frequency count (`self.total`) from the cache file using the `marshal.load` method.<br><br>\nIf loading from the cache file fails (due to an exception), it sets `load_from_cache_fail` to `True`.","metadata":{}},{"cell_type":"code","source":"#             if load_from_cache_fail:\n#                 wlock = DICT_WRITING.get(abs_path, threading.RLock())\n#                 DICT_WRITING[abs_path] = wlock\n#                 with wlock:\n#                     self.FREQ, self.total = self.gen_pfdict(self.get_dict_file())\n#                     default_logger.debug(\n#                         \"Dumping model to file cache %s\" % cache_file)\n#                     try:\n#                         # prevent moving across different filesystems\n#                         fd, fpath = tempfile.mkstemp(dir=tmpdir)\n#                         with os.fdopen(fd, 'wb') as temp_cache_file:\n#                             marshal.dump(\n#                                 (self.FREQ, self.total), temp_cache_file)\n#                         _replace_file(fpath, cache_file)\n#                     except Exception:\n#                         default_logger.exception(\"Dump cache file failed.\")\n\n#                 try:\n#                     del DICT_WRITING[abs_path]\n#                 except KeyError:\n#                     pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If loading from the cache file failed or the cache file is not valid, it proceeds with generating the dictionary data.<br><br>\n`wlock = DICT_WRITING.get` acquires a lock from the `DICT_WRITING` dictionary (or creates one if it doesn't exist) to ensure exclusive access to the dictionary file during initialization.<br><br>\n`with wlock` calls the `gen_pfdict` method to generate the word frequencies (`self.FREQ`) and the total frequency count (`self.total`) by parsing the dictionary file obtained from `self.get_dict_file()`.<br><br>\n`default_logger.debug` logs a message indicating that it's dumping the generated model data to the cache file.","metadata":{}},{"cell_type":"markdown","source":"Inner `try` attempts to create a temporary cache file (`temp_cache_file`) and dumps the generated word frequencies (`self.FREQ`) and the total frequency count (`self.total`) to it using `marshal.dump`.<br><br>\nAfter successfully dumping the data, it uses `_replace_file` to atomically replace the existing cache file with the newly created cache file. This prevents issues when moving files across different filesystems.<br><br>\nIf any exception occurs during this process, it logs an exception message.","metadata":{}},{"cell_type":"markdown","source":"Finally, outer `try` releases the lock by removing the entry from the `DICT_WRITING` dictionary to allow other threads to access the dictionary file.","metadata":{}},{"cell_type":"code","source":"#             self.initialized = True\n#             default_logger.debug(\n#                 \"Loading model cost %.3f seconds.\" % (time.time() - t1))\n#             default_logger.debug(\"Prefix dict has been built successfully.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After successfully initializing the dictionary data, it sets `self.initialized` to `True` to indicate that the dictionary","metadata":{}},{"cell_type":"markdown","source":"### `def check_initialized`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def check_initialized(self):\n#         if not self.initialized:\n#             self.initialize()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It initializes if not initialized.","metadata":{}},{"cell_type":"markdown","source":"### `def calc`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def calc(self, sentence, DAG, route):\n#         N = len(sentence)\n#         route[N] = (0, 0)\n#         logtotal = log(self.total)\n#         for idx in xrange(N - 1, -1, -1):\n#             route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -\n#                               logtotal + route[x + 1][0], x) for x in DAG[idx])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Calculates the route with the maximum probability through the Directed Acyclic Graph (DAG) for word segmentation.\n    - `N` variable stores the length of the sentence.<br> \n    - `route[N]` initializes the route dictionary with an entry for N, which corresponds to the end of the sentence. The entry is a tuple (0, 0), where the first element represents the maximum log probability, and the second element represents the position.<br>\n    - `logtotal` cacluates the logarithm of the total frequency count (`self.total`).<br>\n    This will be used in probability calculations.\n    - `for` loop iterates over the characters in the input sentence in reverse order.<br>\n    Starting from the last character (index `N-1`) and moving backward.\n        - `route[idx]` caculates the log probability of the word from `idx` to `x` , using the formula `log(self.FREQ.get(sentence[idx:x+1]) or 1) - logtotal + route[x+1][0]`.\n        - `self.FREQ.get(sentence[idx:x+1])` retrieves the frequency count of the word from idx to x from the dictionary.<br>If the word is not found (`None`), it defaults to `1`.\n        - It subtracts `logtotal` to normalize the log probabilites.\n        - It adds the log probability of the path to `x+1`, which represents the best path from character `x+1` to the end of the sentence.\n    - The formula uses `max` to find the maximum log probability among all possible paths from `idx` to the end of the sentence.<br>The result of this calculation is a tuple `(max_log_prob, best_position)`.\n    - It stores this tuple in `route` dictionary at index `idx`, representing the best path and maximum probability to reach the end of the sentence starting from character `idx`.","metadata":{}},{"cell_type":"markdown","source":"### `def get_DAG`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def get_DAG(self, sentence):\n#         self.check_initialized()\n#         DAG = {}\n#         N = len(sentence)\n#         for k in xrange(N):\n#             tmplist = []\n#             i = k\n#             frag = sentence[k]\n#             while i < N and frag in self.FREQ:\n#                 if self.FREQ[frag]:\n#                     tmplist.append(i)\n#                 i += 1\n#                 frag = sentence[k:i + 1]\n#             if not tmplist:\n#                 tmplist.append(k)\n#             DAG[k] = tmplist\n#         return DAG","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section constructs a Directed Acyclic Graph (DAG) that represents the possible word combinations in the input sentence.\n- `self.check_initialized()` checks if it is initialized.<br>If not, it initializes it.\n- `for` loop iterates over characters in the input sentence from right to left.\n- `while` loop continues until `i` is less than `N` and `frag` is in `self.FREQ`.\n- `If self.FREQ[frag]` checks if the frequency count of the current `frag` is not zero. <br>If it's not zero, it means that `frag` is a valid word.\n- `i` increases by 1 when each loop finishes, than it repositions to the next word with `sentence[k:i + 1]`.\n- `if not tmplist` checks if `tmplist` is empty.<br>If it is empty, it means that no valid words were found starting from character `k`.\n- `tmplist` is added to `DAG` dictionary with key k.","metadata":{}},{"cell_type":"markdown","source":"### `def __cut_all`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def __cut_all(self, sentence):\n#         dag = self.get_DAG(sentence)\n#         old_j = -1\n#         eng_scan = 0\n#         eng_buf = u''\n#         for k, L in iteritems(dag):\n#             if eng_scan == 1 and not re_eng.match(sentence[k]):\n#                 eng_scan = 0\n#                 yield eng_buf\n#             if len(L) == 1 and k > old_j:\n#                 word = sentence[k:L[0] + 1]\n#                 if re_eng.match(word):\n#                     if eng_scan == 0:\n#                         eng_scan = 1\n#                         eng_buf = word\n#                     else:\n#                         eng_buf += word\n#                 if eng_scan == 0:\n#                     yield word\n#                 old_j = L[0]\n#             else:\n#                 for j in L:\n#                     if j > k:\n#                         yield sentence[k:j + 1]\n#                         old_j = j\n#         if eng_scan == 1:\n#             yield eng_buf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section is used for word segmentation when the `cut_all` method is enabled.\n    - The tokenizer attempts to split the sentence into all possible combinations of words.\n\n- `dag` stores the constructed Directed Acyclic Graph (DAG) that represents the possible word combinations in the input sentence.\n\n- `eng_buf` is an empty unicode string.\n\n- `for` loop iterates through the nodes in the DAG. \n    - `if eng_scan == 1 and not re_eng.match(sentence[K])` checks if the word scan is in progress and the character at index `k` is not part of an English word.\n        - If all the conditions match, it means that the current English word has ended, so it sets `eng_scan` to 0 and yields the English word stored in `eng_buf`.\n    \n    - `if len(L) == 1 and k > old_j` checks if there is only one possible word ending in the current list `L`, and if the current character index `k` is greater than the previous word's ending index `old_j`.\n        - This condition indicates a potential single-character word.\n        - `word = sentence[K:L[0] + 1]` constructs the potential single-character word by taking a swlice of the sentence from character `k` to `L[0] + 1` if the condition is met.\n        - `if re_eng.match(word)` checks if the potential word `word` is an English word by matching it against the `re_eng` regular expression.\n            - `if eng_scan == 0` checks if an English word scan is in progress.<br>If not, it starts a new English word scan by setting `eng_scan` to `1` and stores `word` in `eng_buf`.<br>If English wordscan is already in progress, it appends `word` to the current English word stored in `eng_buf`\n        - It yields word if `eng_scan == 0`, which means the potential word is not an English word, and there was no previous English word scan in progress.\n        - Then set `old_j = L[0]`.<br><br>\n \n    - If `len(L) == 1 and k > old_j` is `False`, it means that there are multiple possible word endings in the list `L`, or the current character is part of a longer word.\n        - `for` loop (`for j in L`) iterates through the possible word endings in `L`.\n            - `if j > k` checks if the ending index `j` is greater than the current character index `k`.<br>If yes, it indicates that there is a valid word ending in this segment of the sentence and yield the word by taking a slice of the sentence from character `k` to `j + 1`, which represents the word.\n            - Then, it updates `old_j` with `j` to store the ending index of the curernt word.\n- After processing the entire DAG, `if eng_scan == 1` checks if an English word scan is still in progress.\n    - If yes, it yields the remaining English word stored in `eng_buf`.\n            ","metadata":{}},{"cell_type":"markdown","source":"### `def __cut_DAG_NO_HMM`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def __cut_DAG_NO_HMM(self, sentence):\n#         DAG = self.get_DAG(sentence)\n#         route = {}\n#         self.calc(sentence, DAG, route)\n#         x = 0\n#         N = len(sentence)\n#         buf = ''\n#         while x < N:\n#             y = route[x][1] + 1\n#             l_word = sentence[x:y]\n#             if re_eng.match(l_word) and len(l_word) == 1:\n#                 buf += l_word\n#                 x = y\n#             else:\n#                 if buf:\n#                     yield buf\n#                     buf = ''\n#                 yield l_word\n#                 x = y\n#         if buf:\n#             yield buf\n#             buf = ''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section is used for word segmentation when Hidden Markov Model is disabled.\n    - `self.calc(sentence, DAG, route)` calculates the best word segmentation path based on the `DAG`.<br>This step populates the route dictionary with information about the optimal word segmentation.\n    - `while` loop iterates through the characters in the `sentence` until `x` is less than the length of the sentence.\n        - `y = route[x][1] + 1` represents the ending index of the current word.\n        - `l_word` represents the current word (from the character at index x to y).\n        - `if re_eng.match(l_word) and len(l_word) == 1` checks if the current word matches an English word by using regular expression, and also verifies that the length of the word is exactly 1.<br>This condition identifies single characters within the text.\n            - If the above condition matches, `l_word` is added to an empty string `buf`.\n            - Then `x` is updated as `y` to proceed to the next sequence.\n        - if the condition above doesn't match, it checks if there are characters stored in the buf variable (indicating an English word in progress).<br>If there are, it yields the English word stored in `buf` and resets buf to an empty string.\n    \n        - `yield l_word` yields the current word, which is either a Chinese word or a non-English character.\n        \n    - `if buf` checks if there are any characters remaining in the `buf` variable.\n        - `yield buf` yields the remaining English word stored in `buf`.\n        - Then it resets the `buf` variable.\n","metadata":{}},{"cell_type":"markdown","source":"### `def __cut_DAG`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def __cut_DAG(self, sentence):\n#         DAG = self.get_DAG(sentence)\n#         route = {}\n#         self.calc(sentence, DAG, route)\n#         x = 0\n#         buf = ''\n#         N = len(sentence)\n#         while x < N:\n#             y = route[x][1] + 1\n#             l_word = sentence[x:y]\n#             if y - x == 1:\n#                 buf += l_word\n#             else:\n#                 if buf:\n#                     if len(buf) == 1:\n#                         yield buf\n#                         buf = ''\n#                     else:\n#                         if not self.FREQ.get(buf):\n#                             recognized = finalseg.cut(buf)\n#                             for t in recognized:\n#                                 yield t\n#                         else:\n#                             for elem in buf:\n#                                 yield elem\n#                         buf = ''\n#                 yield l_word\n#             x = y\n\n#         if buf:\n#             if len(buf) == 1:\n#                 yield buf\n#             elif not self.FREQ.get(buf):\n#                 recognized = finalseg.cut(buf)\n#                 for t in recognized:\n#                     yield t\n#             else:\n#                 for elem in buf:\n#                     yield elem","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This section is used for word segmentation when `HMM` is enabled.<br>This method is responsible for segmenting Chinese into words using `HMM`.\n    - `if y - x == 1` checks if the current word `l_word` consists of a single character.<br>If it is a single character, it means it's a Chinese character.\n    - `if len(buf) == 1` checks if there was a single Chinese character in `buf`.<br>If yes, it yields the single character as standalone word.\n    - When `else` (== `if len(buf) != 1`), it yields the segmented components (segmented by `finalseg.cut(buf)` method) of the unknown word (`if not self.FREQ.get(buf)`).<br>Otherwise, it yields each character in `buf`.\n    - After processing the known words, resets `buf`.<br>Next, it yields `l_word`, which can be multi-character Chinese word or a non-Chinese word.<br>Then, x is updated as y to proceed to the next character position.\n    <br><br>\n    - After the loop is over, `if buf` checks if there is remaining Chinese words in `buf`.\n        - `if len(buf) == 1`, it yields `buf`.\n        - Else if the word in `buf` is not in the frequent list (`elif not self.FREQ,get(buf)`), it yields the segmented components of these unknown words.\n        - `else`, it yields each element in `buf`.","metadata":{}},{"cell_type":"markdown","source":"### `def cut`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def cut(self, sentence, cut_all=False, HMM=True, use_paddle=False):\n#         \"\"\"\n#         The main function that segments an entire sentence that contains\n#         Chinese characters into separated words.\n\n#         Parameter:\n#             - sentence: The str(unicode) to be segmented.\n#             - cut_all: Model type. True for full pattern, False for accurate pattern.\n#             - HMM: Whether to use the Hidden Markov Model.\n#         \"\"\"\n#         is_paddle_installed = check_paddle_install['is_paddle_installed']\n#         sentence = strdecode(sentence)\n#         if use_paddle and is_paddle_installed:\n#             # if sentence is null, it will raise core exception in paddle.\n#             if sentence is None or len(sentence) == 0:\n#                 return\n#             import jieba.lac_small.predict as predict\n#             results = predict.get_sent(sentence)\n#             for sent in results:\n#                 if sent is None:\n#                     continue\n#                 yield sent\n#             return\n#         re_han = re_han_default\n#         re_skip = re_skip_default\n#         if cut_all:\n#             cut_block = self.__cut_all\n#         elif HMM:\n#             cut_block = self.__cut_DAG\n#         else:\n#             cut_block = self.__cut_DAG_NO_HMM\n#         blocks = re_han.split(sentence)\n#         for blk in blocks:\n#             if not blk:\n#                 continue\n#             if re_han.match(blk):\n#                 for word in cut_block(blk):\n#                     yield word\n#             else:\n#                 tmp = re_skip.split(blk)\n#                 for x in tmp:\n#                     if re_skip.match(x):\n#                         yield x\n#                     elif not cut_all:\n#                         for xx in x:\n#                             yield xx\n#                     else:\n#                         yield x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def cut_for_search`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def cut_for_search(self, sentence, HMM=True):\n#         \"\"\"\n#         Finer segmentation for search engines.\n#         \"\"\"\n#         words = self.cut(sentence, HMM=HMM)\n#         for w in words:\n#             if len(w) > 2:\n#                 for i in xrange(len(w) - 1):\n#                     gram2 = w[i:i + 2]\n#                     if self.FREQ.get(gram2):\n#                         yield gram2\n#             if len(w) > 3:\n#                 for i in xrange(len(w) - 2):\n#                     gram3 = w[i:i + 3]\n#                     if self.FREQ.get(gram3):\n#                         yield gram3\n#             yield w","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def lcut`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def lcut(self, *args, **kwargs):\n#         return list(self.cut(*args, **kwargs))\n\n#     def lcut_for_search(self, *args, **kwargs):\n#         return list(self.cut_for_search(*args, **kwargs))\n\n#     _lcut = lcut\n#     _lcut_for_search = lcut_for_search\n\n#     def _lcut_no_hmm(self, sentence):\n#         return self.lcut(sentence, False, False)\n\n#     def _lcut_all(self, sentence):\n#         return self.lcut(sentence, True)\n\n#     def _lcut_for_search_no_hmm(self, sentence):\n#         return self.lcut_for_search(sentence, False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def get_dict_file`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def get_dict_file(self):\n#         if self.dictionary == DEFAULT_DICT:\n#             return get_module_res(DEFAULT_DICT_NAME)\n#         else:\n#             return open(self.dictionary, 'rb')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def load_userdict`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def load_userdict(self, f):\n#         '''\n#         Load personalized dict to improve detect rate.\n\n#         Parameter:\n#             - f : A plain text file contains words and their ocurrences.\n#                   Can be a file-like object, or the path of the dictionary file,\n#                   whose encoding must be utf-8.\n\n#         Structure of dict file:\n#         word1 freq1 word_type1\n#         word2 freq2 word_type2\n#         ...\n#         Word type may be ignored\n#         '''\n#         self.check_initialized()\n#         if isinstance(f, string_types):\n#             f_name = f\n#             f = open(f, 'rb')\n#         else:\n#             f_name = resolve_filename(f)\n#         for lineno, ln in enumerate(f, 1):\n#             line = ln.strip()\n#             if not isinstance(line, text_type):\n#                 try:\n#                     line = line.decode('utf-8').lstrip('\\ufeff')\n#                 except UnicodeDecodeError:\n#                     raise ValueError('dictionary file %s must be utf-8' % f_name)\n#             if not line:\n#                 continue\n#             # match won't be None because there's at least one character\n#             word, freq, tag = re_userdict.match(line).groups()\n#             if freq is not None:\n#                 freq = freq.strip()\n#             if tag is not None:\n#                 tag = tag.strip()\n#             self.add_word(word, freq, tag)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def add_word`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def add_word(self, word, freq=None, tag=None):\n#         \"\"\"\n#         Add a word to dictionary.\n\n#         freq and tag can be omitted, freq defaults to be a calculated value\n#         that ensures the word can be cut out.\n#         \"\"\"\n#         self.check_initialized()\n#         word = strdecode(word)\n#         freq = int(freq) if freq is not None else self.suggest_freq(word, False)\n#         self.FREQ[word] = freq\n#         self.total += freq\n#         if tag:\n#             self.user_word_tag_tab[word] = tag\n#         for ch in xrange(len(word)):\n#             wfrag = word[:ch + 1]\n#             if wfrag not in self.FREQ:\n#                 self.FREQ[wfrag] = 0\n#         if freq == 0:\n#             finalseg.add_force_split(word)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def del_word`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def del_word(self, word):\n#         \"\"\"\n#         Convenient function for deleting a word.\n#         \"\"\"\n#         self.add_word(word, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def suggest_freq`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def suggest_freq(self, segment, tune=False):\n#         \"\"\"\n#         Suggest word frequency to force the characters in a word to be\n#         joined or splitted.\n\n#         Parameter:\n#             - segment : The segments that the word is expected to be cut into,\n#                         If the word should be treated as a whole, use a str.\n#             - tune : If True, tune the word frequency.\n\n#         Note that HMM may affect the final result. If the result doesn't change,\n#         set HMM=False.\n#         \"\"\"\n#         self.check_initialized()\n#         ftotal = float(self.total)\n#         freq = 1\n#         if isinstance(segment, string_types):\n#             word = segment\n#             for seg in self.cut(word, HMM=False):\n#                 freq *= self.FREQ.get(seg, 1) / ftotal\n#             freq = max(int(freq * self.total) + 1, self.FREQ.get(word, 1))\n#         else:\n#             segment = tuple(map(strdecode, segment))\n#             word = ''.join(segment)\n#             for seg in segment:\n#                 freq *= self.FREQ.get(seg, 1) / ftotal\n#             freq = min(int(freq * self.total), self.FREQ.get(word, 0))\n#         if tune:\n#             self.add_word(word, freq)\n#         return freq","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def tokenize`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def tokenize(self, unicode_sentence, mode=\"default\", HMM=True):\n#         \"\"\"\n#         Tokenize a sentence and yields tuples of (word, start, end)\n\n#         Parameter:\n#             - sentence: the str(unicode) to be segmented.\n#             - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n#             - HMM: whether to use the Hidden Markov Model.\n#         \"\"\"\n#         if not isinstance(unicode_sentence, text_type):\n#             raise ValueError(\"jieba: the input parameter should be unicode.\")\n#         start = 0\n#         if mode == 'default':\n#             for w in self.cut(unicode_sentence, HMM=HMM):\n#                 width = len(w)\n#                 yield (w, start, start + width)\n#                 start += width\n#         else:\n#             for w in self.cut(unicode_sentence, HMM=HMM):\n#                 width = len(w)\n#                 if len(w) > 2:\n#                     for i in xrange(len(w) - 1):\n#                         gram2 = w[i:i + 2]\n#                         if self.FREQ.get(gram2):\n#                             yield (gram2, start + i, start + i + 2)\n#                 if len(w) > 3:\n#                     for i in xrange(len(w) - 2):\n#                         gram3 = w[i:i + 3]\n#                         if self.FREQ.get(gram3):\n#                             yield (gram3, start + i, start + i + 3)\n#                 yield (w, start, start + width)\n#                 start += width","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `def set_dictionary`\n-----------------------------------","metadata":{}},{"cell_type":"code","source":"#     def set_dictionary(self, dictionary_path):\n#         with self.lock:\n#             abs_path = _get_abs_path(dictionary_path)\n#             if not os.path.isfile(abs_path):\n#                 raise Exception(\"jieba: file does not exist: \" + abs_path)\n#             self.dictionary = abs_path\n#             self.initialized = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # default Tokenizer instance\n\n# dt = Tokenizer()\n\n# # global functions\n\n# get_FREQ = lambda k, d=None: dt.FREQ.get(k, d)\n# add_word = dt.add_word\n# calc = dt.calc\n# cut = dt.cut\n# lcut = dt.lcut\n# cut_for_search = dt.cut_for_search\n# lcut_for_search = dt.lcut_for_search\n# del_word = dt.del_word\n# get_DAG = dt.get_DAG\n# get_dict_file = dt.get_dict_file\n# initialize = dt.initialize\n# load_userdict = dt.load_userdict\n# set_dictionary = dt.set_dictionary\n# suggest_freq = dt.suggest_freq\n# tokenize = dt.tokenize\n# user_word_tag_tab = dt.user_word_tag_tab\n\n\n# def _lcut_all(s):\n#     return dt._lcut_all(s)\n\n\n# def _lcut(s):\n#     return dt._lcut(s)\n\n\n# def _lcut_no_hmm(s):\n#     return dt._lcut_no_hmm(s)\n\n\n# def _lcut_all(s):\n#     return dt._lcut_all(s)\n\n\n# def _lcut_for_search(s):\n#     return dt._lcut_for_search(s)\n\n\n# def _lcut_for_search_no_hmm(s):\n#     return dt._lcut_for_search_no_hmm(s)\n\n\n# def _pcut(sentence, cut_all=False, HMM=True):\n#     parts = strdecode(sentence).splitlines(True)\n#     if cut_all:\n#         result = pool.map(_lcut_all, parts)\n#     elif HMM:\n#         result = pool.map(_lcut, parts)\n#     else:\n#         result = pool.map(_lcut_no_hmm, parts)\n#     for r in result:\n#         for w in r:\n#             yield w\n\n\n# def _pcut_for_search(sentence, HMM=True):\n#     parts = strdecode(sentence).splitlines(True)\n#     if HMM:\n#         result = pool.map(_lcut_for_search, parts)\n#     else:\n#         result = pool.map(_lcut_for_search_no_hmm, parts)\n#     for r in result:\n#         for w in r:\n#             yield w\n\n\n# def enable_parallel(processnum=None):\n#     \"\"\"\n#     Change the module's `cut` and `cut_for_search` functions to the\n#     parallel version.\n\n#     Note that this only works using dt, custom Tokenizer\n#     instances are not supported.\n#     \"\"\"\n#     global pool, dt, cut, cut_for_search\n#     from multiprocessing import cpu_count\n#     if os.name == 'nt':\n#         raise NotImplementedError(\n#             \"jieba: parallel mode only supports posix system\")\n#     else:\n#         from multiprocessing import Pool\n#     dt.check_initialized()\n#     if processnum is None:\n#         processnum = cpu_count()\n#     pool = Pool(processnum)\n#     cut = _pcut\n#     cut_for_search = _pcut_for_search\n\n\n# def disable_parallel():\n#     global pool, dt, cut, cut_for_search\n#     if pool:\n#         pool.close()\n#         pool = None\n#     cut = dt.cut\n#     cut_for_search = dt.cut_for_search","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# $Below\\ In\\ Progress$\n","metadata":{}},{"cell_type":"markdown","source":"# Combining Columns","metadata":{}},{"cell_type":"code","source":"cn_new = pd.DataFrame()\nen_new = pd.DataFrame()\ncn_new['input'] = \"Headline: \" + cn_example['headline'] + \"; Content: \" + cn_example['content']\nen_new['input'] = \"Headline: \" + en_example['headline'] + \"; Content: \" + en_example['content']","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.325382Z","iopub.status.idle":"2023-09-12T12:30:44.325741Z","shell.execute_reply.started":"2023-09-12T12:30:44.325562Z","shell.execute_reply":"2023-09-12T12:30:44.325578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cn_new[cn_new['input'].isna()==True]\ncn_new = cn_new.dropna()\ncn_new[cn_new['input'].isna()==True]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.326823Z","iopub.status.idle":"2023-09-12T12:30:44.327182Z","shell.execute_reply.started":"2023-09-12T12:30:44.327007Z","shell.execute_reply":"2023-09-12T12:30:44.327030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"en_new[en_new['input'].isna()==True]\nen_new = en_new.dropna()\nen_new[en_new['input'].isna()==True]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.328306Z","iopub.status.idle":"2023-09-12T12:30:44.328657Z","shell.execute_reply.started":"2023-09-12T12:30:44.328489Z","shell.execute_reply":"2023-09-12T12:30:44.328506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(cn_new.head(5))\ndisplay(en_new.head(5))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.330013Z","iopub.status.idle":"2023-09-12T12:30:44.330368Z","shell.execute_reply.started":"2023-09-12T12:30:44.330200Z","shell.execute_reply":"2023-09-12T12:30:44.330216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization\nTokenization is the process of breaking down a text into smaller units, which are typically words or subwords.<br>These smaller units are called tokens.<br><br>\nIn English, tokenization usually involves splitting text into words based on spaces, punctuation, or other delimiters.<br>\nFor example, the sentence \"I love ice cream\" would be tokenized into the tokens: [\"I\", \"love\", \"ice\", \"cream\"].<br><br>\nIn languages like Chinese, tokenization can be more complex since <br>Tokenization might involve segmenting text into characters or meaningful subword units.\n<br>\n\n### Trying Out SentencePiece\nGot the [Colab Doc](https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=SUcAbKnRVAv6), but not sure how to use it yet.<br>\nAs always, [abhishek](https://www.kaggle.com/abhishek)'s [notebook](https://www.kaggle.com/code/abhishek/sentencepiece-tokenizer-with-offsets/notebook) helped me a lot on SentencePiece implementation.","metadata":{}},{"cell_type":"code","source":"class SentencePieceTokenizer:\n    '''\n    from Abhishek Thakur's notebook\n    https://www.kaggle.com/code/abhishek/sentencepiece-tokenizer-with-offsets\n    '''\n    def __init__(self, model_path):\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load(model_path +'.model')\n        \n    def encode(self, sentence):\n        spt = sentencepiece_pb2.SentencePieceText()\n        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n        offsets = []\n        tokens = []\n        for piece in spt.pieces:\n            tokens.append(piece.id)\n            offsets.append((piece.begin, piece.end))\n        return tokens, offsets","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.331721Z","iopub.status.idle":"2023-09-12T12:30:44.332117Z","shell.execute_reply.started":"2023-09-12T12:30:44.331898Z","shell.execute_reply":"2023-09-12T12:30:44.331913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deberta_tok = AutoTokenizer.from_pretrained('/kaggle/input/debertav3base')\ndeberta_tok.save_pretrained('/kaggle/working/deberta_tok/')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.333141Z","iopub.status.idle":"2023-09-12T12:30:44.333475Z","shell.execute_reply.started":"2023-09-12T12:30:44.333313Z","shell.execute_reply":"2023-09-12T12:30:44.333329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spt = SentencePieceTokenizer('/kaggle/input/debertav3base/spm')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.334536Z","iopub.status.idle":"2023-09-12T12:30:44.334876Z","shell.execute_reply.started":"2023-09-12T12:30:44.334709Z","shell.execute_reply":"2023-09-12T12:30:44.334725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\nHere I have tried on encoding different words, languages and tokens.<br>\nLet's see if it works well!<br>","metadata":{}},{"cell_type":"markdown","source":"#### Tokens\nIt is fun to see how these tokens are treated *similarly*","metadata":{}},{"cell_type":"code","source":"print(f\"[MASK] encoded into \\t{spt.encode('[MASK]')}\")\nprint(f\"[CLS] encoded into \\t{spt.encode('[CLS]')}\")\nprint(f\"[EOS] encoded into \\t{spt.encode('[EOS]')}\")\nprint(f\"[UNK] encoded into \\t{spt.encode('[UNK]')}\")\nprint(f\"[SEP] encoded into \\t{spt.encode('[SEP]')}\")\nprint(f\"[SPECIAL] encoded into \\t{spt.encode('[SPECIAL]')}\")\nprint()\nprint(f\"MASK encoded into \\t{spt.encode('MASK')}\")\nprint(f\"CLS encoded into \\t{spt.encode('CLS')}\")\nprint(f\"EOS encoded into \\t{spt.encode('EOS')}\")\nprint(f\"UNK encoded into \\t{spt.encode('UNK')}\")\nprint(f\"SEP encoded into \\t{spt.encode('SEP')}\")\nprint(f\"SPECIAL encoded into \\t{spt.encode('SPECIAL')}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.335803Z","iopub.status.idle":"2023-09-12T12:30:44.336184Z","shell.execute_reply.started":"2023-09-12T12:30:44.335993Z","shell.execute_reply":"2023-09-12T12:30:44.336013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"[MASK] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[MASK]', add_special_tokens=False)}\")\nprint(f\"\\n[CLS] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[CLS]', add_special_tokens=False)}\")\nprint(f\"\\n[EOS] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[EOS]', add_special_tokens=False)}\")\nprint(f\"\\n[UNK] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[UNK]', add_special_tokens=False)}\")\nprint(f\"\\n[SEP] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[SEP]', add_special_tokens=False)}\")\nprint(f\"\\n[SPECIAL] encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('[SPECIAL]', add_special_tokens=False)}\")\nprint()\nprint(f\"\\nMASK encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('MASK', add_special_tokens=False)}\")\nprint(f\"\\nCLS encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('CLS', add_special_tokens=False)}\")\nprint(f\"\\nEOS encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('EOS', add_special_tokens=False)}\")\nprint(f\"\\nUNK encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('UNK', add_special_tokens=False)}\")\nprint(f\"\\nSEP encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('SEP', add_special_tokens=False)}\")\nprint(f\"\\nSPECIAL encoded into :\\nvvvvvvvvvvvvvvvvvvvvvvvvvv\\n{deberta_tok('SPECIAL', add_special_tokens=False)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.337390Z","iopub.status.idle":"2023-09-12T12:30:44.337740Z","shell.execute_reply.started":"2023-09-12T12:30:44.337567Z","shell.execute_reply":"2023-09-12T12:30:44.337583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### English words\nWe can see that these sample English words are encoded properly *(maybe?)*","metadata":{}},{"cell_type":"code","source":"display(spt.encode(en_new.input[0][:10]))\ndisplay(spt.encode(en_new.input[0][:15]))\ndisplay(spt.encode(en_new.input[0][:30]))\ndisplay(spt.encode(en_new.input[0][:50]))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.338845Z","iopub.status.idle":"2023-09-12T12:30:44.339203Z","shell.execute_reply.started":"2023-09-12T12:30:44.339035Z","shell.execute_reply":"2023-09-12T12:30:44.339052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(deberta_tok(en_new.input[0][:10], add_special_tokens=False))\ndisplay(deberta_tok(en_new.input[0][:15], add_special_tokens=False))\ndisplay(deberta_tok(en_new.input[0][:30], add_special_tokens=False))\ndisplay(deberta_tok(en_new.input[0][:50], add_special_tokens=False))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.340169Z","iopub.status.idle":"2023-09-12T12:30:44.340509Z","shell.execute_reply.started":"2023-09-12T12:30:44.340340Z","shell.execute_reply":"2023-09-12T12:30:44.340357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chinese words\nNow the problem begins, the last digit of the encoded tensors different.<br> *(which I think is the bytes taken)*<br>\nWhich implies that this doesn't work at all for Chinese words.<br><br>\n**edit**<br>\nWait what...?<br>\nLast time I checked, there were no differences between 你 and 您.<br>\nBut when I check it now, there is a difference...<br>\nIt seems like deberta is also working well for Chinese characters.","metadata":{}},{"cell_type":"code","source":"display(spt.encode(cn_new.input[0][:10]))\ndisplay(spt.encode(cn_new.input[0][:15]))\ndisplay(spt.encode(cn_new.input[0][:30]))\ndisplay(spt.encode(cn_new.input[0][:40]))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.341445Z","iopub.status.idle":"2023-09-12T12:30:44.341776Z","shell.execute_reply.started":"2023-09-12T12:30:44.341613Z","shell.execute_reply":"2023-09-12T12:30:44.341628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(deberta_tok(cn_new.input[0][:10], add_special_tokens=False))\ndisplay(deberta_tok(cn_new.input[0][:15], add_special_tokens=False))\ndisplay(deberta_tok(cn_new.input[0][:30], add_special_tokens=False))\ndisplay(deberta_tok(cn_new.input[0][:40], add_special_tokens=False))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.342731Z","iopub.status.idle":"2023-09-12T12:30:44.343083Z","shell.execute_reply.started":"2023-09-12T12:30:44.342893Z","shell.execute_reply":"2023-09-12T12:30:44.342908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So I tried... and this works!!<br>\n**edit** I will leave below as it is.<br>","metadata":{}},{"cell_type":"code","source":"cn_spt = SentencePieceTokenizer('/kaggle/input/sentencepiece-chinese-bpe/chinese/chinese')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.343843Z","iopub.status.idle":"2023-09-12T12:30:44.344236Z","shell.execute_reply.started":"2023-09-12T12:30:44.344054Z","shell.execute_reply":"2023-09-12T12:30:44.344073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(cn_spt.encode(cn_new.input[0][:10]))\ndisplay(cn_spt.encode(cn_new.input[0][:15]))\ndisplay(cn_spt.encode(cn_new.input[0][:30]))\ndisplay(cn_spt.encode(cn_new.input[0][:40]))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.346566Z","iopub.status.idle":"2023-09-12T12:30:44.347131Z","shell.execute_reply.started":"2023-09-12T12:30:44.346827Z","shell.execute_reply":"2023-09-12T12:30:44.346852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Korean words\nSame here, I am pretty... no very sure that this doesn't work on foreign languages.<br><br>\n**edit**<br>\nOops.. again, this seems like it works well...!","metadata":{}},{"cell_type":"code","source":"display(spt.encode('나'))\ndisplay(spt.encode('제'))\nprint()\ndisplay(spt.encode('안녕'))\ndisplay(spt.encode('안녕하세요'))\nprint()\ndisplay(spt.encode('이름'))\ndisplay(spt.encode('이름은'))\nprint()\ndisplay(spt.encode('이희상'))\ndisplay(spt.encode('이희상입니다'))\nprint()\ndisplay(spt.encode('안녕하세요 제 이름은'))\ndisplay(spt.encode('안녕하세요 제 이름은 이희상입니다'))\ndisplay(spt.encode('안녕하세요. 제 이름은 이희상입니다. 홍대에서 공부를 하고 있습니다.'))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.348265Z","iopub.status.idle":"2023-09-12T12:30:44.348768Z","shell.execute_reply.started":"2023-09-12T12:30:44.348511Z","shell.execute_reply":"2023-09-12T12:30:44.348534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(deberta_tok('나', add_special_tokens=False))\ndisplay(deberta_tok('제', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('안녕', add_special_tokens=False))\ndisplay(deberta_tok('안녕하세요', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('이름', add_special_tokens=False))\ndisplay(deberta_tok('이름은', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('이희상', add_special_tokens=False))\ndisplay(deberta_tok('이희상입니다', add_special_tokens=False))\nprint()\ndisplay(deberta_tok('안녕하세요 제 이름은', add_special_tokens=False))\ndisplay(deberta_tok('안녕하세요 제 이름은 이희상입니다', add_special_tokens=False))\ndisplay(deberta_tok('안녕하세요. 제 이름은 이희상입니다. 홍대에서 공부를 하고 있습니다.', add_special_tokens=False))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.350363Z","iopub.status.idle":"2023-09-12T12:30:44.350864Z","shell.execute_reply.started":"2023-09-12T12:30:44.350607Z","shell.execute_reply":"2023-09-12T12:30:44.350631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, I tried as below!<br>\nIt seems like it is working properly! :)<br><br>\n\n**edit**<br>\nI will leave below as it is.<br>","metadata":{}},{"cell_type":"code","source":"ko_spt = SentencePieceTokenizer('/kaggle/input/airc-keti-ke-t5/vocab/sentencepiece_v2')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.352007Z","iopub.status.idle":"2023-09-12T12:30:44.352537Z","shell.execute_reply.started":"2023-09-12T12:30:44.352284Z","shell.execute_reply":"2023-09-12T12:30:44.352308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(ko_spt.encode('나'))\ndisplay(ko_spt.encode('제'))\nprint()\ndisplay(ko_spt.encode('안녕'))\ndisplay(ko_spt.encode('안녕하세요'))\nprint()\ndisplay(ko_spt.encode('이름'))\ndisplay(ko_spt.encode('이름은'))\nprint()\ndisplay(ko_spt.encode('이희상'))\ndisplay(ko_spt.encode('이희상입니다'))\nprint()\ndisplay(ko_spt.encode('안녕하세요 제 이름은'))\ndisplay(ko_spt.encode('안녕하세요 제 이름은 이희상입니다'))\ndisplay(ko_spt.encode('안녕하세요. 제 이름은 이희상입니다. 홍대에서 공부를 하고 있습니다.'))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:30:44.354079Z","iopub.status.idle":"2023-09-12T12:30:44.354574Z","shell.execute_reply.started":"2023-09-12T12:30:44.354321Z","shell.execute_reply":"2023-09-12T12:30:44.354344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}